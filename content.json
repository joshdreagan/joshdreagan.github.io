{"pages":[{"title":"Tags","text":"","link":"/tags/"}],"posts":[{"title":"ActiveMQ HA Performance Comparison","text":"A while back, I wrote a blog post on HA Deployments With Fuse. Surprisingly, it received a lot of interest. In particular, the JMS section. Apparently, there are tons of people trying to solve the cross-dc HA problem… Not surprisingly, everyone claims to have the highest possible failure requirements and asked me what I meant by “prepare to make some serious performance tradeoffs”. So I figured I’d give some concrete numbers for comparison. Before we get started though, here are some very important disclaimers: First, I did absolutely no tweaking of options or performance tuning. I just unzipped the Red Hat JBoss A-MQ distro, uncommented a user, and started it up. For the DRBD tests, I only changed the path where it stores the KahaDB to point to the replicated filesystem mount. Similarly, I just did a standard DRBD installation by following their getting started docs. I did not tweak or otherwise optimize the synchronization protocols in any way. Second, I used an ‘m4.xlarge’ EC2 instance type and attached a standard SSD EBS volume type. So more of a mid-level machine with average (or below) networking and storage performance. Given the above conditions, you should not take these numbers as a performance metric. You could likely get much greater performance if you tweaked some of the network settings and used more performant machine. These numbers are only meant to serve as a comparison of local storage vs replicated storage. Now that I’ve got the “disclaimers” out of the way, let me run down what I actually did… First, I wanted to run a baseline (since this is not about performance numbers directly, but rather comparison). So I created an ‘m4.xlarge’ instance used to host my broker. We’ll call it “broker 1”. I also created a second ‘m4.xlarge’ instance in the same availability zone and region to host my client (both producer and consumer). We’ll call it “client”. Next, I installed Red Hat JBoss A-MQ 6.3 on “broker 1”. I also created a Maven project for my ActiveMQ Perf Maven Plugin on “client”. I then ran a client instance with 5 threads for the producer, and another instance with 5 threads for the consumer. Note: Both clients were run from the same “client” EC2 machine. I then repeated the test using 25 threads, and again using 50 threads. I tried to go above 50 threads, but the performance started to degrade. Likely due to a network or storage bottleneck for the machine/storage types I chose. All the numbers for these runs can be found in the table below in the “Baseline” row. Once I had my baseline, I spun up another ‘m4.xlarge’ instance in the same region, but on another availability zone. We’ll call this one “broker 2” I also created and attached an EBS volume (using a standard SSD) to both of my “broker” instances (“broker 1” and “broker 2”). I installed DRBD on the instances and configured it to replicate synchronously (using Protocol C) from “broker 1” to “broker 2”. Once I brought them online and the initial sync was done, I ran the same tests as above and captured the numbers. The numbers can be found in the table below in the “DRBD (across availability zones)” row. As you can see from the results, we do get a performance drop (because we have to write the data twice), but it’s not too bad since we’re not having to replicate across a WAN yet. Finally, I terminated the “broker 2” instance and recreated it in another region. So now, the “broker 1” and “client” instances were in the “US West (Oregon)” region. And the “broker 2” instance was in the “US East (N. Virginia)” region. I reconnected DRBD to replicate from “broker 1” to “broker 2” and waited for the initial sync to complete (this took a while). Once that was done, I re-ran the same tests and captured the output again. It can be found in the table below in the “DRBD (across regions)” row. Now we see the “performance tradeoffs” I was talking about… In my tests, it was between 1-2 orders of magnitude slower than local storage. So I will repeat my statement from my previous blog… “You will not be processing large sets of data while synchronously replicating across a WAN.” Performance Results 5 Threads 25 Threads 50 Threads Baseline 2229 tps 9456 tps 12524 tps DRBD (across availability zones) 1636 tps 7068 tps 9133 tps DRBD (across regions) 26 tps 129 tps 256 tps For reference, here are the links to the actual ActiveMQ Perf Maven Plugin run outputs: 5 Threads Baseline Producer Baseline Consumer DRBD Producer (across availability zones) DRBD Consumer (across availability zones) DRBD Producer (across regions) DRBD Consumer (across regions) 25 Threads Baseline Producer Baseline Consumer DRBD Producer (across availability zones) DRBD Consumer (across availability zones) DRBD Producer (across regions) DRBD Consumer (across regions) 50 Threads Baseline Producer Baseline Consumer DRBD Producer (across availability zones) DRBD Consumer (across availability zones) DRBD Producer (across regions) DRBD Consumer (across regions)","link":"/2017/03/15/activemq_ha_performance_comparison/"},{"title":"Artemis Disaster Recovery","text":"A few years back, I wrote a blog about HA strategies for Fuse/AMQ. In it, I talked about a few potential solutions for setting up a DR (Disaster Recovery) configuration for ActiveMQ. The most popular solution, it seems, was to utilize block-level disk replication software. Since it’s been quite a while, and since we’ve updated our messaging code base to Apache Artemis, I figured I’d write up a new post with yet another potential architecture. Before I begin, I’d like to state that the previous solution that I had outlined, though written for ActiveMQ instead of Artemis, still applies. If you want to guarantee absolutely no loss or duplication of data, you must replicate things synchronously. So if that’s your requirement, my recommendation would still be to use a block-level disk replication software like LINBIT DRBD or Red Hat Ceph. However, because these solutions are fully synchronous, they can perform a bit slowly. In addition, there are other issues that you need to account for. For instance, what do you do if the backup site is down? Do you keep the primary site running and have the backup “catch up” once it comes online? If so, then you risk the loss or duplication of data if you suffer an outage while the two are out-of-sync. If not, then you have two potential points of failure that would cause a total outage as an outage of either DC would mean you’d have to stop processing. But what if I’m willing to give up a little bit of that “absolute” guarantee in order to get a solution that performs well? For that, I’ll need to switch over to a solution that’s asynchronous in nature (or “near real-time” as Oracle likes to call it). That should take care of my performance issues. But what about the the other scenario where my backup site is down? I’ll need a way to buffer up the data so that, when the backup site comes back online, I can finish sync’ing and not drop anything. Finally, it would also be nice to be able to fail back once the primary comes back online. That means that I’ll need to sync data from primary to backup, but also from backup to primary. So what would such a solution look like in Artemis? Well, sort like this: Let’s break it down… First, we can use a feature known as “diverts” to wiretap off a local copy of our messages into one or more buffer queues. Then, we can use something called a “core bridge” to forward the buffered messages to an address on a secondary site. Technically, using these features, we can have as many backup sites as we’d like. We’re only limited by our available bandwidth. So mirroring of data is actually quite simple. At least in concept… One issue that we will encounter is that, because we’re forwarding to/from the same named address on each data center, the messages will continue to divert and forward around in an endless loop. Not ideal. What we really need is some way to selectively divert messages so that we’ll only copy/forward messages that originated at our DC. Any other messages would still be processed, but we would assume that they were sent from another DC and thus we would not need to forward them around. Luckily, diverts include the ability to filter. So we can simply add a header stating the origin DC and then use that to filter out any messages that did not originate in our DC. That means that we just need all of our clients to include that header, and we’ve solved our circular forwarding issue. But what if I don’t control or can’t change my clients to add that header? Well, core bridges allow you to specify a message transformer. So we can use that to automatically “stamp” the messages as they’re forwarded. Neat! So now we’ve got the mirroring portion done. What’s next? Ok… A mirror is just a copy of the data. So that means that when we failover to our backup, we’ll process all the same data again resulting in duplicates. This might be ok if your application code handles it. If so, you can stop here. This is all you need. If, however, your application code does not handle duplicates, we’ll need a solution to filter out messages that we’ve already processed. For that, of course, I’ll use Camel. :) Camel has an EIP known as Idempotent Consumer that will keep track of processed IDs and skip them if you try to process them again. Perfect! We’ll just use that… It even gives me the option to plug in whatever idempotent repository implementation I’d like. There are several to choose from. Hopefully the next issue is obvious at this point. If I’ve got a local (to my DC) repository keeping track of processed message IDs, how do I get that information over to my other DC. What I really need is a way to add something to my local ID repo, and also mirror it off to my other DC so it’s added in that DCs local repo as well. There are lots of ways to do this (ie, Oracle GoldenGate, Debezium, Ceph, DRBD, …), but I’m already replicating my message data in artemis using diverts and core bridges. So why don’t I just do that here as well? Basically, I just need to create a custom idempotent repository implementation that updates its local repo, but also sends a notification to an Artemis address. That message will be diverted to a buffer queue, then forwarded across with a core bridge. Then I just need one more component that picks those messages up and adds them to the local ID repo. Easy! So now I have a solution that will mirror data between DCs. I can mirror between as many as I’d like. I can fail over and fail back. I can detect and skip duplicates. So what’s the catch? Well, because my replication is asynchronous, I can’t run my consumer in more than one DC at a time. If I do, I will run the risk of processing duplicates since the messages might replicate and get processed before the idempotent ID notification. But since this is a DR solution, that’s probably not too big of a deal. Technically though, I lied a bit. A more accurate statement would be that I can’t run a consumer for a given queue on more than once DC at a time. That means that, if I have several queues, I can spread my consumers across DCs evenly. If I need to, I can fail them over to another DC, and then fail them back when ready. But I’m partitioning the processing of my individual queues across my DCs. This is known as “partitioned active/active” and it means I get to utilize the hardware on all my DCs instead of having them just sit around as backups. Awesome! All of this is fine and good. But how about some code? Here’s a link to a repo where I’ve implemented this solution: [https://github.com/joshdreagan/artemis-async-dr]. There are instructions for running it on bare metal (or AWS, or whatever), as well as instructions for running it on OpenShift. Enjoy!","link":"/2019/10/14/artemis_disaster_recovery/"},{"title":"AMQP Performance Testing With JBoss A-MQ","text":"I recently had a customer that wanted us to do some load testing of Red Hat’s JBoss A-MQ for them. In particular, this customer wanted the tests performed using the AMQP protocol instead of ActiveMQ’s native OpenWire. From previous engagements, I knew that there would be a performance difference. But after a quick look I didn’t see any blogs or posts on the subject. More specifically, I didn’t see any posts that detailed how to run the tests yourself so that you could get real numbers in your own environment. So I figured I’d write up some steps and post my results for future reference. Please note that the purpose of this blog post is not to give you a number of msg/s to expect or tell you the absolute best way to tune your broker. The purpose of this post is to show you how to run the tests yourself and give you some very rough idea of the performance difference between the two protocols. All of my testing was done on my laptop using the default configurations. You will probably get wildly different performance in your environment and will likely need to tune the broker specific to your use case to get the best performance possible. Protocol BasicsBoth AMQP (Advanced Message Queuing Protocol) and OpenWire are what we refer to as “wire-level protocols”. They define how a client and broker will negotiate a connection, how they’ll format messages, and how they’ll communicate in general. ActiveMQ can speak many different wire-level protocols, but OpenWire and AMQP are supposed to be the fastest as they are both binary in nature. In both cases, you have multiple options as to what language your clients can be written in (ie, Java, .NET, C++, …). It really doesn’t matter as long as they are communicating via the same wire-level protocol as the broker. In this case, I’ll be using Java simply because of the availability of test tools. Test DetailsThere are a ton of available test tools that you can use to drive traffic to your brokers. For this blog, I used the ActiveMQ Perf Maven Plugin. This is a great tool that comes with the ActiveMQ source code and allows you to run as both producers and consumers. It is easily configurable and gives you a ton of options to test various scenarios (ie, persistent/non-persistent, transacted/non-transacted, 1-n producer/consumer threads, …). The only caveat to the tool is that it will load a connection factory that uses the OpenWire protocol. Luckily, it was written in such a way that we can extend it to use AMQP as well. To do that, we only need to create one class that knows how to load an AMQP connection factory. The source for the class is as follows: 1234567891011package org.jboss.examples.amqp.spi;import org.apache.activemq.tool.spi.ReflectionSPIConnectionFactory;public class AMQPReflectionSPIConnectionFactory extends ReflectionSPIConnectionFactory { @Override public String getClassName() { return &quot;org.apache.qpid.jms.JmsConnectionFactory&quot;; }} Then we just need to make sure that our custom class and the Qpid client libraries are on the Maven classpath. 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;${project.groupId}&lt;/groupId&gt; &lt;artifactId&gt;${project.artifactId}&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-amqp&lt;/artifactId&gt; &lt;version&gt;${activemq.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.qpid&lt;/groupId&gt; &lt;artifactId&gt;qpid-jms-client&lt;/artifactId&gt; &lt;version&gt;${qpid.version}&lt;/version&gt;&lt;/dependency&gt; The tricky part is that we need them on the classpath of the Maven plugin itself and not necessarily the classpath of the project. Take a look at the pom.xml file to see what I mean. The full code for this test can be found at https://github.com/joshdreagan/amqp-perf-test. Feel free to just clone it down and use it directly. If you take a look at the plugin’s documentation, you’ll see several properties that can be set for the test as a whole, the consumer, the producer, and the connection factory. You can configure the properties using “-Dkey=value” arguments on the command line, or via a .properties file (or some combination of the two). If you plan to run the test multiple times, it’s probably worth creating a .properties file. The only detail that I’ll give here is that the connection factory properties are set via reflection and will be specific to the connection factory used. I mention this because we swap out the connection factory when we run the AMQP tests. To give a concrete example, when using the org.apache.activemq.tool.spi.ActiveMQReflectionSPI you will set the URI for the broker using -Dfactory.brokerURL=tcp://localhost:61616, but when using the org.jboss.examples.amqp.spi.AMQPReflectionSPIConnectionFactory you will set the URI for the broker using -Dfactory.remoteURI=amqp://localhost:5672. This is because the “setters” are named differently for the different connection factory implementations. The available options for the AMQP connection factory can be found in the Qpid docs. All other options (ie, producer, consumers, &amp; test) should be the same and are found on the plugin docs. Test ResultsHere are some sample runs that I did. Please note the disclaimer at the beginning of this blog post regarding performance results. Producer OpenWire12345678910111213141516171819202122232425262728293031323334353637383940414243$&gt; mvn activemq-perf:producer -DsysTest.propsConfigFile=src/main/resources/tcp-producer.propertiesOpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=2048m; support was removed in 8.0[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building ActiveMQ Perf: AMQP Perf Test 1.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO][INFO] --- activemq-perf-maven-plugin:5.11.0:producer (default-cli) @ amqp-perf-test ---[INFO] Loading properties file: /home/jreagan/Development/Projects/joshdreagan/amqp-perf-test/src/main/resources/tcp-producer.properties[INFO] Created: org.apache.activemq.ActiveMQConnectionFactory using SPIConnectionFactory: org.apache.activemq.tool.spi.ActiveMQReflectionSPI[INFO] Sampling duration: 300000 ms, ramp up: 0 ms, ramp down: 0 ms[INFO] Creating queue: queue://TEST.FOO[INFO] Creating JMS Connection: Provider=ActiveMQ-5.11.0, JMS Spec=1.1[INFO] Creating producer to: queue://TEST.FOO with non-persistent delivery.[INFO] Starting to publish 1024 byte(s) messages for 300000 ms[INFO] Finished sending[INFO] Client completed############################################# SYSTEM THROUGHPUT SUMMARY #############################################System Total Throughput: 13347982System Total Clients: 1System Average Throughput: 44493.27333333333System Average Throughput Excluding Min/Max: 44274.73333333333System Average Client Throughput: 44493.27333333333System Average Client Throughput Excluding Min/Max: 44274.73333333333Min Client Throughput Per Sample: clientName=JmsProducer0, value=0Max Client Throughput Per Sample: clientName=JmsProducer0, value=65562Min Client Total Throughput: clientName=JmsProducer0, value=13347982Max Client Total Throughput: clientName=JmsProducer0, value=13347982Min Average Client Throughput: clientName=JmsProducer0, value=44493.27333333333Max Average Client Throughput: clientName=JmsProducer0, value=44493.27333333333Min Average Client Throughput Excluding Min/Max: clientName=JmsProducer0, value=44274.73333333333Max Average Client Throughput Excluding Min/Max: clientName=JmsProducer0, value=44274.73333333333[INFO] Created performance report: /home/jreagan/Development/Projects/joshdreagan/amqp-perf-test/./target/JmsProducer_numClients1_numDests1_all.xml[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 05:10 min[INFO] Finished at: 2016-02-02T11:46:30-07:00[INFO] Final Memory: 8M/235M[INFO] ------------------------------------------------------------------------ Consumer OpenWire123456789101112131415161718192021222324252627282930313233343536373839404142$&gt; mvn activemq-perf:consumer -DsysTest.propsConfigFile=src/main/resources/tcp-consumer.propertiesOpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=2048m; support was removed in 8.0[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building ActiveMQ Perf: AMQP Perf Test 1.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO][INFO] --- activemq-perf-maven-plugin:5.11.0:consumer (default-cli) @ amqp-perf-test ---[INFO] Loading properties file: /home/jreagan/Development/Projects/joshdreagan/amqp-perf-test/src/main/resources/tcp-consumer.properties[INFO] Created: org.apache.activemq.ActiveMQConnectionFactory using SPIConnectionFactory: org.apache.activemq.tool.spi.ActiveMQReflectionSPI[INFO] Sampling duration: 300000 ms, ramp up: 0 ms, ramp down: 0 ms[INFO] Creating queue: queue://TEST.FOO[INFO] Creating JMS Connection: Provider=ActiveMQ-5.11.0, JMS Spec=1.1[INFO] Creating non-durable consumer to: queue://TEST.FOO[INFO] Starting to asynchronously receive messages for 300000 ms...[INFO] Client completed############################################# SYSTEM THROUGHPUT SUMMARY #############################################System Total Throughput: 13287310System Total Clients: 1System Average Throughput: 44291.03333333333System Average Throughput Excluding Min/Max: 44074.76System Average Client Throughput: 44291.03333333333System Average Client Throughput Excluding Min/Max: 44074.76Min Client Throughput Per Sample: clientName=JmsConsumer0, value=0Max Client Throughput Per Sample: clientName=JmsConsumer0, value=64882Min Client Total Throughput: clientName=JmsConsumer0, value=13287310Max Client Total Throughput: clientName=JmsConsumer0, value=13287310Min Average Client Throughput: clientName=JmsConsumer0, value=44291.03333333333Max Average Client Throughput: clientName=JmsConsumer0, value=44291.03333333333Min Average Client Throughput Excluding Min/Max: clientName=JmsConsumer0, value=44074.76Max Average Client Throughput Excluding Min/Max: clientName=JmsConsumer0, value=44074.76[INFO] Created performance report: /home/jreagan/Development/Projects/joshdreagan/amqp-perf-test/./target/JmsConsumer_numClients1_numDests1_equal.xml[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 05:00 min[INFO] Finished at: 2016-02-02T11:46:19-07:00[INFO] Final Memory: 7M/220M[INFO] ------------------------------------------------------------------------ Producer AMQP123456789101112131415161718192021222324252627282930313233343536373839404142434445$&gt; mvn activemq-perf:producer -DsysTest.propsConfigFile=src/main/resources/amqp-producer.propertiesOpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=2048m; support was removed in 8.0[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building ActiveMQ Perf: AMQP Perf Test 1.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO][INFO] --- activemq-perf-maven-plugin:5.11.0:producer (default-cli) @ amqp-perf-test ---[INFO] Loading properties file: /home/jreagan/Development/Projects/joshdreagan/amqp-perf-test/src/main/resources/amqp-producer.properties[INFO] Created: org.apache.qpid.jms.JmsConnectionFactory using SPIConnectionFactory: org.jboss.examples.amqp.spi.AMQPReflectionSPIConnectionFactory[INFO] Sampling duration: 300000 ms, ramp up: 0 ms, ramp down: 0 ms[INFO] Creating queue: queue://TEST.FOO[INFO] Best match for SASL auth was: SASL-PLAIN[INFO] Connection ID:bearkat-34938-1454439352850-0:2 connected to remote Broker: amqp://localhost:5672[INFO] Creating JMS Connection: Provider=QpidJMS-0.5.0, JMS Spec=1.1[INFO] Creating producer to: TEST.FOO with non-persistent delivery.[INFO] Starting to publish 1024 byte(s) messages for 300000 ms[INFO] Finished sending[INFO] Client completed############################################# SYSTEM THROUGHPUT SUMMARY #############################################System Total Throughput: 3600145System Total Clients: 1System Average Throughput: 12000.483333333334System Average Throughput Excluding Min/Max: 11947.51System Average Client Throughput: 12000.483333333334System Average Client Throughput Excluding Min/Max: 11947.51Min Client Throughput Per Sample: clientName=JmsProducer0, value=1800Max Client Throughput Per Sample: clientName=JmsProducer0, value=14092Min Client Total Throughput: clientName=JmsProducer0, value=3600145Max Client Total Throughput: clientName=JmsProducer0, value=3600145Min Average Client Throughput: clientName=JmsProducer0, value=12000.483333333334Max Average Client Throughput: clientName=JmsProducer0, value=12000.483333333334Min Average Client Throughput Excluding Min/Max: clientName=JmsProducer0, value=11947.51Max Average Client Throughput Excluding Min/Max: clientName=JmsProducer0, value=11947.51[INFO] Created performance report: /home/jreagan/Development/Projects/joshdreagan/amqp-perf-test/./target/JmsProducer_numClients1_numDests1_all.xml[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 05:00 min[INFO] Finished at: 2016-02-02T12:00:53-07:00[INFO] Final Memory: 10M/128M[INFO] ------------------------------------------------------------------------ Consumer AMQP1234567891011121314151617181920212223242526272829303132333435363738394041424344$&gt; mvn activemq-perf:consumer -DsysTest.propsConfigFile=src/main/resources/amqp-consumer.propertiesOpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=2048m; support was removed in 8.0[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building ActiveMQ Perf: AMQP Perf Test 1.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO][INFO] --- activemq-perf-maven-plugin:5.11.0:consumer (default-cli) @ amqp-perf-test ---[INFO] Loading properties file: /home/jreagan/Development/Projects/joshdreagan/amqp-perf-test/src/main/resources/amqp-consumer.properties[INFO] Created: org.apache.qpid.jms.JmsConnectionFactory using SPIConnectionFactory: org.jboss.examples.amqp.spi.AMQPReflectionSPIConnectionFactory[INFO] Sampling duration: 300000 ms, ramp up: 0 ms, ramp down: 0 ms[INFO] Creating queue: queue://TEST.FOO[INFO] Best match for SASL auth was: SASL-PLAIN[INFO] Connection ID:bearkat-51184-1454439351110-0:2 connected to remote Broker: amqp://localhost:5672[INFO] Creating JMS Connection: Provider=QpidJMS-0.5.0, JMS Spec=1.1[INFO] Creating non-durable consumer to: TEST.FOO[INFO] Starting to asynchronously receive messages for 300000 ms...[INFO] Client completed############################################# SYSTEM THROUGHPUT SUMMARY #############################################System Total Throughput: 3583507System Total Clients: 1System Average Throughput: 11945.023333333333System Average Throughput Excluding Min/Max: 11899.69System Average Client Throughput: 11945.023333333333System Average Client Throughput Excluding Min/Max: 11899.69Min Client Throughput Per Sample: clientName=JmsConsumer0, value=0Max Client Throughput Per Sample: clientName=JmsConsumer0, value=13600Min Client Total Throughput: clientName=JmsConsumer0, value=3583507Max Client Total Throughput: clientName=JmsConsumer0, value=3583507Min Average Client Throughput: clientName=JmsConsumer0, value=11945.023333333333Max Average Client Throughput: clientName=JmsConsumer0, value=11945.023333333333Min Average Client Throughput Excluding Min/Max: clientName=JmsConsumer0, value=11899.69Max Average Client Throughput Excluding Min/Max: clientName=JmsConsumer0, value=11899.69[INFO] Created performance report: /home/jreagan/Development/Projects/joshdreagan/amqp-perf-test/./target/JmsConsumer_numClients1_numDests1_equal.xml[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 05:01 min[INFO] Finished at: 2016-02-02T12:00:51-07:00[INFO] Final Memory: 12M/243M[INFO] ------------------------------------------------------------------------ That’s it! You can see that OpenWire is quite a bit faster in this case. But please don’t take my word for it… clone the project, tweak some configurations, try scaling the producers/consumers/brokers, and run the test yourself to see what results you’ll get.","link":"/2016/02/02/amqp_performance_testing/"},{"title":"Bridging Apache Artemis","text":"In a perfect world, every project would be a greenfield project and you could pick and choose the latest/greatest broker every time. And of course, if given the choice you’d pick Apache Artemis… ;) But in the real world, it is much more common that customers will have a very large existing codebase and will have to transition to the latest/greatest over some much longer period of time. And during that period, it’s helpful to be able to set up some sort of bridge between your brokers so that your old and new code can still communicate. Before we get started though, there are a few things to clear up… The term “bridge” can mean may things. Artemis has something called a “Core Bridge“. Core bridges do not use the JMS API, and are used only for bridging Artemis instances together (either explicitly, or implicitly when using cluster connections). While useful, Core bridges are not the focus of this post. Instead, we’re going to talk about how you might bridge Artemis to another JMS broker (ie, IBM MQ). There is a baked in JMS bridge implementation in the Artemis codebase that you can use if you’d like. You can check out the docs on it here: [https://activemq.apache.org/artemis/docs/latest/jms-bridge.html]. It’s pretty basic, but functional if you don’t require any kind of transformations or other flexibility. To use it, simply instantiate an instance of org.apache.activemq.artemis.jms.bridge.JMSBridge, and pass it the required arguments (ie, source/destination org.apache.activemq.artemis.jms.bridge.ConnectionFactoryFactory instances, source/destination org.apache.activemq.artemis.jms.bridge.DestinationFactory instances, source/destination credentials, …). Once you have your instance, you can call start() on it to begin consuming/producing messages. There’s even an example included in the distribution: [jms-bridge]. Seems pretty straightforward right? Well… maybe not. If you’re running Artemis standalone (like most people), you don’t really have a good way to load extra code. That is, there’s no “hot deploy” folder where you can just drop a jar file with this config in it and have it automatically startup and shutdown with the broker. There has been some effort to create a plugin framework in the newer versions, but a quick look at the code showed that it still doesn’t provide hooks into start/stop or other broker lifecycle events. So what do we do? As it turns out, there is an embedded Jetty instance that serves the web admin console. And it already starts/stops with the server. So we can just piggyback on that to bootstrap our code. But if I’m going through all of this effort to bootstrap some code, I might as well use something with a bit more power (ie, Apache Camel). That way, if I have any need to perform message transformations, set unique headers for idempotent consumption, wiretap audit logs, or do anything else outside of simply copying messages, I can do so. Sounds simple enough, but how do I actually do it? First, you’ll need to create a WAR containing all of your dependencies as well as your Spring XML defined Camel routes. We’ll refer to this as jms-bridge.war… In the Camel routes, you’ll place your actual JMS bridge definitions (as many as you need). In the WEB-INF/web.xml, you’ll boostrap the Spring ApplicationContext using the org.springframework.web.context.ContextLoaderListener class. This is a pretty standard way to bootstrap a Spring application into a servlet container. Second, you’ll copy jms-bridge.war file into the $ARTEMIS_HOME/web folder. Finally, you’ll add a definition to the $ARTEMIS_INSTANCE/etc/bootstrap.xml file as shown below: 1234567&lt;broker xmlns=&quot;http://activemq.org/schema&quot;&gt; ... &lt;web bind=&quot;http://localhost:8161&quot; path=&quot;web&quot;&gt; ... &lt;app url=&quot;jms-bridge&quot; war=&quot;jms-bridge.war&quot;/&gt; &lt;/web&gt;&lt;/broker&gt; Notice the use of $ARTEMIS_HOME and $ARTEMIS_INSTANCE in the second and third steps… $ARTEMIS_HOME refers to the location that you unzipped the Artemis distribution. $ARTEMIS_INSTANCE refers to the directory of your broker instance (created with the “artemis create ...“ command. Now when you start the broker instance, it will load up and start the Camel routes and begin bridging. Pretty cool! But we bundled up all of our dependencies inside our WAR file (ie, all of the IBM MQ JMS client libs). And we also bundled our Camel routes definition inside the WAR as well. And since the WAR file sits in the $ARTEMIS_HOME/web directory, its configurations apply to any and all of the instances that we create/run on that machine. Not to mention the fact that every change to my Camel routes will require a build/deploy/restart. All of this kind of sucks and we can do better… If you dig into the code a bit more, you’ll see that the “main” that spins up the Artemis broker will actually add several directories, JARs, &amp; ZIPs to the classpath. Specifically, it will add the $ARTEMIS_HOME/etc &amp; $ARTEMIS_INSTANCE/etc directories. It will then scan through the $ARTEMIS_HOME/lib &amp; $ARTEMIS_INSTANCE/lib directories, and add any JARs or ZIPs that it finds to the classpath (sorted by name). So that means that we can take all of our use-case specific stuff (ie, all of the IBM MQ JMS client libs) and put them in the $ARTEMIS_INSTANCE/lib folder. It also means that we can take the Camel routes configuration, and put it in the $ARTEMIS_INSTANCE/etc directory. Now we have a completely generic WAR that we deploy in the $ARTEMIS_HOME/lib directory. That WAR can be activated on an instance defined basis as needed. And each instance can have the dependencies and configuration that it requires completely independent of other instances. Additionally, I have the added benefit of being able to edit the Camel routes configuration and apply my changes with only a restart (ie, no re-build/re-deploy required). Neat! Want to see what it looks like? Take a look at this sample project: [https://github.com/joshdreagan/artemis-jms-bridge].","link":"/2017/08/16/bridging_apache_artemis/"},{"title":"Bridging Apache Artemis - Part 2","text":"In a previous post, I wrote about setting up a bridge between Apache Artemis and another JMS broker. Specifically in that case, I used IBM MQ. For part 2, I’d like to cover something a little more complex. Bridging between Apache Artemis and Apache Kafka. To be clear, bridging from Artemis to Kafka doesn’t have to be difficult. As always, it depends on your use case. Let’s talk about the simplest one first. Uni-directional Let’s say you’re simply trying to bridge in one direction. For example, imagine you have a bunch of IoT devices spread around various sites. These types of devices typically don’t run a Kafka client. More commonly, they’ll speak one of the many standardized protocols like MQTT. If those devices are simply publishing telemetry data, then the communication is only flowing in one direction. In this case, you’d only need a couple of components. First, you’ll want to run an Artemis broker since it speaks MQTT. You could run the Artemis instance anywhere really, but most likely you’d want to run it on each site (close to the devices). This gives you several benefits, such as insulating your sites from network failures and simplifying security. Second, you’d need a simple Camel bridge application to consume the messages from each of the Artemis edge brokers, and produce them to the centralized Kafka cluster. And finally, you’ll obviously need a Kafka cluster. Take a look at the picture below for an example. Also, you can poke around the code if you want to see it running: https://github.com/joshdreagan/iot-demo Bi-directional (simple) Now let’s complicate things a bit and try to do bi-directional communication. In this case, let’s assume that maybe those IoT devices running out on the edge might want to receive command messages in addition to publishing their telemetry data. You’d of course need all the same components as before. The tricky part is routing the messages where they need to go. You might be tempted to simply use a hierarchical addressing strategy like you would normally do with MQTT. Something like “commands.US.CO.site-1234.device-5678” (would be “/commands/US/CO/site-1234/device-5678” in MQTT speak) to indicate the region and device you’d like to issue the command to. That way I can have independent Camel bridges (one for each Artemis/site) that consume from Kafka the topics that they’re interested in (potentially using wildcards), and produce to their local Artemis broker that’s colocated with the target device. This strategy is not bad if you have a limited number of devices, but issues can arise when you maybe have millions of devices. While Kafka is great with raw throughput, it doesn’t really like having a ton of topics (and partitions). The move from Zookeeper to KRaft has improved this quite a bit, but you still need to try to keep the number of topics/partitions relatively low. So what do we do? Well, one solution might be to use that same hierarchical address structure, but move some of the information into a header. Using the previous example, maybe something like “commands.US.CO.site-1234” for the address, and put the “device-5678” in a message header. This would allow you to cut down the number of Kafka topics to be equal to the number of sites you have, as opposed to the total number of devices across all sites. And it would still allow the Camel bridge application to easily route messages to the correct broker and target device by concatenating the headers and source topic to rebuild the full address that it would produce to on its local Artemis broker. This strategy has its limits though. If you were to move any more of the address into a header, the routing that the Camel bridge application has to do becomes much more complicated. Following the same example, let’s say I used “commands.US.CO” for the address, and put both “site-1234” and “device-5678” into headers. Now my site-local Camel bridge applications can no longer assume that all messages on the Kafka topic are of interest to them. And Kafka has no concept like JMS message selectors. So we either have the option of pulling messages we don’t care about and filtering locally, or we have to move to a more centralized push model. The obvious negative about local filtering is that it potentially greatly increases unnecessary network traffic. In my example, every bridge application in Colorado would pull all messages for every site in that state. Even for sites it’s not responsible for. Depending on the number of sites, and number of command messages, this could be a lot of wasted bandwidth. And the other option is not much better. If we moved the Camel bridge application up to the hub, and made it responsible for bridging to all sites in Colorado, we would eliminate the wasted bandwidth issue. However, our Camel application now has to be smart enough to know the addresses of all Artemis brokers at all sites in the state, and keep a routing map of which site (ie, “site-1234”) maps to which Artemis broker URL. So doable, but not ideal. Bi-directional (complex) Now, let’s cover a really complex case. What if we have bi-directional communication, but it’s not data that can be broken up by address? What if, instead, I want to have competing consumers, and am using Artemis as more of a proxy to Kafka just so my clients can speak AMQP (or whatever protocol), or be coded to the JMS API? Well, much like the previous cases, producing is not a problem. As messages come in to Artemis, I can simply use a bridge to pick them up and push them to a Kafka cluster. I can even be slick about it and use a feature called “diverts” to redirect all messages that I want to forward onto a single “outbound” queue. That way my bridge only has to consume from that one queue. All information about the original address a message was produced to is stored in headers. So bridging to the appropriate Kafka topic is easy. But what about consuming? That’s actually the difficult part. Let’s assume that I have more than one Artemis broker. Let’s also assume that my consumers are spread across those brokers. If I just used a simple Kafka-&gt;Artemis Camel bridge, it would pull down all messages (as fast as it could) and produce them to whatever Artemis broker it was connected to. That Artemis broker might not actually have a consumer attached to it. The consumer might be attached to one of the other Artemis brokers. So now I’d need to store-and-forward them around an Artemis cluster. That’s extra hops to persistence, and more wasted bandwidth. What if, instead, I only pulled messages across when a consumer was actually attached and listening on a given queue/topic? That way I could avoid unnecessary hops, and potentially (depending on my use case) avoid clustering the Artemis brokers altogether. In addition, it would reduce the amount of storage I would need on my Artemis brokers as I could now control how many messages I pull across at a time. Unfortunately, in order to have hooks into Artemis-internal things like consumers attaching/listening, I won’t be able to use a simple external Camel application. For that type of insight, I’d need to implement my bridge as a broker plugin that runs inside the Artemis broker itself. Using this strategy, we can handle both queues and topics. With a few caveats… First, for queues, you’d need all the bridge instances to use the same Kafka consumer group ID. Which limits the number of bridges you can have to be equal to the number of partitions the underlying Kafka topic has. This most likely wouldn’t be too big of a problem since each Artemis instance can handle thousands of clients. And if I have thousands of partitions, I can have thousands of Artemis brokers/bridges/proxies, with each one handling thousands of clients. So probably good enough. For topics, I would want to configure each bridge instance to have a unique Kafka consumer group ID. So in that case, I wouldn’t be limited by the number of partitions, but rather by the fact that each of my bridge consumers would effectively be single threaded. Which may or may not be a performance bottleneck. The second issue is that Kafka consumers pull messages in batches for performance reasons. So you could potentially run into a case where a consumer attaches to an Artemis broker, then a bridge is fired up and starts pulling messages, but the consumer goes away before it consumes the full batch. In that case, you’d either have messages that were stuck on a specific Artemis broker waiting for a consumer to come back, or you’d fail the batch and potentially get duplicates when the consumer reattaches and the batch is pulled again. Which of those you get depends on your “ack” strategy for the KafkaConsumer. You could, of course, mitigate this by lowering your batch size on your KafkaConsumer. But doing so would impact your performance. In my opinion, it would be better to make your consumer apps tolerant of duplicates (since duplicates are inevitable in literally any system anyway). I did implement an example of this strategy, which can be found here: https://github.com/joshdreagan/artemis-kafka-bridge/. However, it could certainly use some improvement. For example, right now I have the producer and consumer queues separated so that I could use Artemis’s built-in queue-depth blocking to limit the number of messages I pull at a time. This could be replaced with some custom blocking code, which (when combined with selective diverts) would allow it to then use the same queue name for both producer and consumer. Which would make it completely transparent to clients. Also, right now the queues/topics/diverts have to be manually created. With a little more coding, the diverts could be automatically created and applied by matching a wildcard on the address. Which would greatly simplify the configuration necessary to use it. But all of that is for another day when I maybe get some more free time… Which is code for “probably never”. 😂 Summary So which option is right for you? Easy… the simplest one that meets your requirements. Whatever they may be.","link":"/2025/06/19/bridging_apache_artemis_part_2/"},{"title":"Calling Native Code With Camel","text":"Usually when a customer has some legacy/native code that they need to invoke from Camel (or any Java program really), I recommend that they expose it via SOAP, REST, or some other standardized remote invocation mechanism. Then they can just call it with the appropriate Camel Component. While I still think that this is best option, I recently had a customer ask if Camel had the ability to call native code directly. So I figured what the heck… might as well blog about it… Native code can be written in many languages (obviously…). And unfortunately, the mechanisms that you would use to invoke the compiled binaries (.dll‘s or .so‘s) are not always the same. In this blog post, I’m going to cover what I think are the most common (or at least the most commonly requested) native languages and how to invoke them. COf the languages that I’ll cover in this post, calling C libaries is definitely the easiest. Once I have my compiled library (.dll or .so), I can just call it using JNI or JNA. Of the two, I prefer JNA since it doesn’t require me to generate any code using javah or really have much knowledge of the C code itself. All I need to do is create a Java interface that extends com.sun.jna.Library and that matches the signature of the C library (or more specifically its methods). Then I can create a proxy instance automatically using the com.sun.jna.Native#loadLibrary(java.lang.String, java.lang.Class) method (passing the name of your library and the interface class). Note: You don’t give the full path or name of the library. The name is automatically wrapped with the platform-specific parts and the path is looked up via a variety of mechanisms (ie, the jna.library.path system property). Once I have a proxy object, I can invoke methods on it like I would with any Java Bean in Camel using the Bean Component. Fairly straighforward right? Take a look at this project (specifically the “camel-native-c” module) if you’d like to see it all working: https://github.com/joshdreagan/camel-native. C++Invoking C++ code is slightly more involved than C code, but is still pretty simple. The options (ie, JNI or JNA) are the same, and for the most part the code is the same. If you choose to use JNA (which I did), you still need to define an interface that matches the method signatures of the C++ library (and implements com.sun.jna.Library). And you still need to load the library and create the proxy object using the com.sun.jna.Native class. The differences/complications come from this thing called Name Mangling. Name mangling is something that C++ compilers do to account for namespaces and overloaded methods and such. Most compilers do specify exactly how they perform the mangling, but unfortunately none of them do it the same. That is, the Windows C++ compiler will mangle the names differently than the GNU C++ compiler. At any rate, the end result is that my C++ method will not have the same name in its compiled form as it did in its source form. For example, if I had a method called add in my source, the GNU compiler might turn it into something like _ZN10Calculator3addEii (depending on the namespace and method signature of course) (see note below). That means that when I try to invoke the method on my com.sun.jna.Library interface, I won’t have the right name (and will get an exception). So how do I get around all this name mangling tomfoolery? Well, luckily, JNA allows us to provide a custom com.sun.jna.FunctionMapper. That means I can tell JNA that, when I call a function named add, it really needs to call a funtion named _ZN10Calculator3addEii in the underlying native library. Neat! One more oddity to handle… For whatever reason, the name mangling also adds an extra argument to the method signature. But once again, JNA has a mechanism to fix it. We can create a custom com.sun.jna.InvocationMapper to intercept the invocation and add a null argument to the beginning of the arg list. That’s it! Now I can invoke it using the Bean Component just like I did in the C example. Like I said… slightly more involved, but still pretty simple. To see it all in action, take a look at this project (specifically the “camel-native-cpp” module): https://github.com/joshdreagan/camel-native. Note: You can use the nm tool on Linux to find the mangled names (ie, nm -D libc-calculator.so). C#C# (or any .NET code) is probably the most complicated case. On Windows, .NET DLLs are not the same format as a truely native DLL. This is because they aren’t really native code. They’re what Microsoft calls Managed Code. Managed code is code that must be run in a virtual machine. Think Java and its JVM, but more Microsofty… So it can’t be invoked directly (or at least not without loading a virtual machine). That means that using JNI or JNA directly are out. So what do I do? Well, there are a few options. If my .NET DLL exposed a COM interface, I could write a C++ wrapper that calls the .NET DLL through COM. Then I could follow the above instructions for invoking C++ code. But what if my library didn’t expose a COM interface? In my experience, most don’t. And even if it did, that seems like a lot of effort, and I’m quite lazy… I could perhaps use a paid tool like javOnet or JNBridge. Both seem simple enough to use and are probaly pretty stable. But they both cost a lot of money, and I’m quite cheap… So what’s left? I went with a project called jni4net. It does all the work for me (no writing wrapper code), and the price is right (free!). Plus, it’s the only one of the options I found that was open source (and I like open source! :)). Now for the bad news… It’s not the most full-featured or simplest tool in the world. That’s ok though. My cheapness typically wins out over my laziness. So here goes… First we need to generate and build the wrapper code. Sadly, the tool has a few steps to its build (making scripting a little complex), but it’s not too bad. You have to run the proxygen.exe command line tool (located in the bin directory of the distribution) and point it at your .NET DLL to generate the Java and C# wrapper code. This process also generates a build.cmd file that must then be used to actually build the code into a DLL library and its respective JAR file. This obviously doesn’t really mesh well with the way most Java builds are written. But with a bit of cleverness/ugliness, we can make it work. Now that we have some generated/compiled wrapper code, we need to initialize what jni4net calls the “bridge” and register the assemblies. This must be done at startup before any of the generated code/methods are used. A simple “initializer” class that calls the net.sf.jni4net.Bridge#init() and net.sf.jni4net.Bridge#LoadAndRegisterAssemblyFrom(java.io.File) methods will do the trick. Once this is done, we can invoke the generated wrapper class methods using the Bean Component just like in the previous examples. All done right? Not quite… If you tried to this yourself, you probably noticed that there are all kinds of exceptions that occur when you actually try to run your project. This stems from a bit of quirkiness with the way that jni4net finds and loads the DLLs and JARs. As it turns out, the tool is quite opinionated about where its libraries live (making running in production potentially tricky). What do I mean? In order to run, the location of the jni4net JAR files (both generated and the ones that come with the distribution) must live in the same directory as your DLL files (both generated and the .NET one you’re trying to make use of). A quick look at the code showed that this is because it uses a combination of path and naming conventions to figure out the locations of the supporting libs. Unfortunately, this is currently hard-coded. :( Fortunately, this is an open source project! So anyone with a bit of free time could fix this and submit a pull request. :) All in all it’s a bit cumbersome, but works. And that’s what really matters right? For an actual running example, take a look at this project (specifically the “camel-native-csharp” module): https://github.com/joshdreagan/camel-native.","link":"/2016/11/21/calling_native_code_with_camel/"},{"title":"Camel Aggregation Strategies","text":"One of the many (many many many) extension points inside Apache Camel is the org.apache.camel.processor.aggregate.AggregationStrategy. These are used in everything from Content Enrichers to Splitters to Aggregators and more. Since their use is so prevalent, I figured that I’d dedicate a whole blog post just for them. So here goes… So what are AggregationStrategy’s anyway? Simple… they’re implementations of the org.apache.camel.processor.aggregate.AggregationStrategy that allow you to specify exactly how two exchanges will be merged. This specification can be as simple or as complex as you require for your use case. Maybe you just want to take the first response and ignore all others. Maybe you want to combine the XML bodies into a list and then merge a select few headers. The limit really is your imagination. But what do I mean by “merging exchanges”? Let’s take a look at a few concrete examples. Out of the BoxFor starters, there are several implementations that are included out of the box. You can use them “as-is” without writing any custom code at all. Let’s talk through a few of them with some potential use cases. The first is the org.apache.camel.processor.aggregate.UseLatestAggregationStrategy implementation. It’s the default strategy for most Camel EIPs that accept aggregation strategies. So if you don’t specify any strategy, this is likely the one you’re using. Basically, it takes the last exchange it receives and just uses that (ignoring any others that may have been aggregated prior). One example use case for this would be when doing an Aggregator. Perhaps you’re receiving many messages as input, but you want to buffer them (giving the user time to send in corrections/updates), and then only send the latest message to the backend after some period of inactivity. That might look like below: 123456789101112&lt;bean id=&quot;useLatest&quot; class=&quot;org.apache.camel.processor.aggregate.UseLatestAggregationStrategy&quot;/&gt;&lt;camelContext xmlns=&quot;http://activemq.apache.org/camel/schema/spring&quot;&gt; &lt;route&gt; &lt;from uri=&quot;direct:acceptUpdateableRequest&quot;/&gt; &lt;aggregator strategyRef=&quot;useLatest&quot; completionTimeout=&quot;5000&quot;&gt; &lt;correlationExpression&gt; &lt;header&gt;UniqueRequestID&lt;/header&gt; &lt;/correlationExpression&gt; &lt;to uri=&quot;direct:bufferedSendToBackend&quot;/&gt; &lt;/aggregator&gt; &lt;/route&gt;&lt;/camelContext&gt; For the next use case, we’ll cover the (very similar) org.apache.camel.processor.aggregate.UseOriginalAggregationStrategy implementation. As the name would suggest, it “merges” two exchanges together by completely ignoring the new exchange and just taking the original. One example of where this might be useful is when doing a Multicast. Lets say I wanted to send a copy of a message off to multiple recipients, but really don’t care about their response. After the multicast is completed, I want to perform some transformation on the original message, and then return the result. Instead of rolling my own implementation, I could simply use the one provided. Something like this: 1234567891011&lt;bean id=&quot;useOriginal&quot; class=&quot;org.apache.camel.processor.aggregate.UseOriginalAggregationStrategy&quot;/&gt;&lt;camelContext xmlns=&quot;http://activemq.apache.org/camel/schema/spring&quot;&gt; &lt;route&gt; &lt;from uri=&quot;direct:acceptRequest&quot;/&gt; &lt;multicast strategyRef=&quot;useOriginal&quot;&gt; &lt;to uri=&quot;direct:recipient1&quot;/&gt; &lt;to uri=&quot;direct:recipient2&quot;/&gt; &lt;/multicast&gt; &lt;to uri=&quot;xslt:transformOriginal.xsl&quot;/&gt; &lt;/route&gt;&lt;/camelContext&gt; The next set of implementations, I’ll cover as a group. They are the org.apache.camel.processor.aggregate.GroupedExchangeAggregationStrategy, org.apache.camel.processor.aggregate.GroupedMessageAggregationStrategy, and org.apache.camel.processor.aggregate.GroupedBodyAggregationStrategy strategies. They will combine the exchanges into a list and then pass the list itself along to the next processor. They only differ by what they put in the list (ie, List&lt;Exchange&gt;, List&lt;Message&gt;, or List&lt;Object&gt;). So, for instance, if you wanted to split a message, process each individual part, and then combine the individual results back into a list, you could do so easily using a Splitter like below: 12345678910&lt;bean id=&quot;listOfBody&quot; class=&quot;org.apache.camel.processor.aggregate.GroupedBodyAggregationStrategy&quot;/&gt;&lt;camelContext xmlns=&quot;http://activemq.apache.org/camel/schema/spring&quot;&gt; &lt;route&gt; &lt;from uri=&quot;direct:acceptListRequestExpectingListResponse&quot;/&gt; &lt;split strategyRef=&quot;listOfBody&quot;&gt; &lt;simple&gt;${body}&lt;/simple&gt; &lt;to uri=&quot;direct:sendIndividualRequest&quot;/&gt; &lt;/split&gt; &lt;/route&gt;&lt;/camelContext&gt; The final implementation that I’ll cover for this section is the org.apache.camel.util.toolbox.XsltAggregationStrategy. It allows you to provide an XSLT that will be used to merge the original and new exchanges together. A great use case for this is when you want to Enrich an XML request with some extra data retrieved from a backend. 123456789101112&lt;bean id=&quot;xsltEnrichmentStrategy&quot; class=&quot;org.apache.camel.util.toolbox.XsltAggregationStrategy&quot;&gt; &lt;constructor-arg value=&quot;/META-INF/xslt/EnrichIndexHtml.xsl&quot;/&gt;&lt;/bean&gt;&lt;camelContext xmlns=&quot;http://activemq.apache.org/camel/schema/spring&quot;&gt; &lt;route&gt; &lt;from uri=&quot;direct:acceptRequest&quot;/&gt; &lt;to uri=&quot;language:constant:classpath:/META-INF/html/index.html&quot;/&gt; &lt;enrich strategyRef=&quot;xsltEnrichmentStrategy&quot;&gt; &lt;constant&gt;direct:fetchCds&lt;/constant&gt; &lt;/enrich&gt; &lt;/route&gt;&lt;/camelContext&gt; Since this example is a little more complex, it requires more than just a code snippet to explain. So I’ve put together an example application and thrown it up on GitHub. Take a look… https://github.com/joshdreagan/camel-xslt-enricher It’s amazing how many use cases these “canned” aggregation strategies cover. But what if I they’re not quite exactly what you need? Semi-CustomIn this section, we’ll discuss what I call “semi-custom” strategies. Basically, they’re base/utility classes that make it easy for you to implement a custom strategy with very little Java code. The first class we’ll talk about is the org.apache.camel.processor.aggregate.AbstractListAggregationStrategy. Similar to the grouping implementations mentioned above, the end result of this strategy is a list of items. The difference is that you have total control over what data gets placed in said list as well as where you pull it from. Here’s a very simple example implementation: 123456789101112package org.apache.camel.examples;import org.apache.camel.Exchange;import org.apache.camel.processor.aggregate.AbstractListAggregationStrategy;public class SimpleListAggregationStrategy extends AbstractListAggregationStrategy&lt;String&gt; { @Override public String getValue(Exchange exchange) { return exchange.getIn().getHeader(&quot;MyAwesomeHeader&quot;, String.class); }} If you need even more control over the aggregation, you can use the org.apache.camel.util.toolbox.FlexibleAggregationStrategy. The FlexibleAggregationStrategy is a fluent strategy builder that lets you define fairly complex aggregation strategy implementations using a very concise syntax. If you’re using the Java DSL to define your Camel routes (or are using any Java based bean wiring mechanism), you can just use the fluent builder directly. However, if you’re using it from the Spring DSL (using Spring’s XML bean definitions) it might be easier to wrapper it in a simple Java implementation. See below for an example: 1234567891011121314151617181920212223package org.apache.camel.examples;import org.apache.camel.Exchange;import org.apache.camel.model.language.SimpleExpression;import org.apache.camel.processor.aggregate.AggregationStrategy;import org.apache.camel.util.toolbox.AggregationStrategies;public class CorrelationIdAggregationStrategy implements AggregationStrategy { private final AggregationStrategy delegate; public FluentAggregationStrategy() { delegate = AggregationStrategies.flexible() .storeInHeader(&quot;MyCorrelationID&quot;) .pick(new SimpleExpression(&quot;${body}&quot;)) ; } @Override public Exchange aggregate(Exchange oldExchange, Exchange newExchange) { return delegate.aggregate(oldExchange, newExchange); }} You could then use your implementation like this: 1234567891011121314&lt;bean id=&quot;uuidEnrichmentStrategy&quot; class=&quot;org.apache.camel.examples.CorrelationIdAggregationStrategy&quot;/&gt;&lt;camelContext xmlns=&quot;http://activemq.apache.org/camel/schema/spring&quot;&gt; &lt;route&gt; &lt;from uri=&quot;direct:acceptRequest&quot;/&gt; &lt;enrich strategyRef=&quot;uuidEnrichmentStrategy&quot;&gt; &lt;constant&gt;direct:fetchUuid&lt;/constant&gt; &lt;/enrich&gt; &lt;/route&gt; &lt;route&gt; &lt;from uri=&quot;direct:fetchUuid&quot;/&gt; &lt;bean beanType=&quot;java.util.UUID&quot; method=&quot;randomUUID&quot;/&gt; &lt;convertBodyTo type=&quot;java.lang.String&quot;/&gt; &lt;/route&gt;&lt;/camelContext&gt; Pretty powerful stuff! But what if you’re feeling even more imaginative? CustomThe last type of strategy that I’ll talk about is a “completely custom” implementation. This basically just means that you will implement the org.apache.camel.processor.aggregate.AggregationStrategy interface directly without using any helper base classes (which might restrict you in some ways). Because of this direct implementation, you are free to do literally anything you want. One example that I whipped up for a customer a while back is what I called the “semi-streaming aggregation strategy”. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package org.apache.camel.examples;import java.util.Comparator;import java.util.Objects;import java.util.SortedSet;import java.util.TreeSet;import org.apache.camel.CamelContext;import org.apache.camel.CamelContextAware;import org.apache.camel.Exchange;import org.apache.camel.Message;import org.apache.camel.Processor;import org.apache.camel.RuntimeCamelException;import org.apache.camel.processor.aggregate.AggregateProcessor;import org.apache.camel.processor.aggregate.AggregationStrategy;import org.apache.camel.util.ExchangeHelper;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.InitializingBean;public class SemiStreamingAggregationStrategy implements AggregationStrategy, CamelContextAware, InitializingBean { private static final Logger log = LoggerFactory.getLogger(SemiStreamingAggregationStrategy.class); public static final String LAST_PROCESSED_INDEX = &quot;CamelAggregatorLastProcessedIndex&quot;; private String aggregateProcessorId; private CamelContext camelContext; private String sequenceIdHeaderName; // Lazily initialized. private AggregateProcessor _aggregateProcessor; private Comparator&lt;Message&gt; _messageComparator; public String getAggregateProcessorId() { return aggregateProcessorId; } public void setAggregateProcessorId(String aggregateProcessorId) { this.aggregateProcessorId = aggregateProcessorId; } @Override public void setCamelContext(CamelContext camelContext) { this.camelContext = camelContext; } @Override public CamelContext getCamelContext() { return camelContext; } public String getSequenceIdHeaderName() { return sequenceIdHeaderName; } public void setSequenceIdHeaderName(String sequenceIdHeaderName) { this.sequenceIdHeaderName = sequenceIdHeaderName; } protected AggregateProcessor _aggregateProcessor() { if (_aggregateProcessor == null) { _aggregateProcessor = camelContext.getProcessor(aggregateProcessorId, AggregateProcessor.class); } return _aggregateProcessor; } protected Comparator&lt;Message&gt; _messageComparator() { if (_messageComparator == null) { _messageComparator = (Message t, Message t1) -&gt; t.getHeader(sequenceIdHeaderName, Comparable.class).compareTo(t1.getHeader(sequenceIdHeaderName, Comparable.class)); } return _messageComparator; } @Override public void afterPropertiesSet() throws Exception { Objects.requireNonNull(aggregateProcessorId, &quot;The aggregateProcessorId property must not be null.&quot;); Objects.requireNonNull(camelContext, &quot;The camelContext property must not be null.&quot;); Objects.requireNonNull(sequenceIdHeaderName, &quot;The sequenceIdHeaderName property must not be null.&quot;); } @Override public Exchange aggregate(Exchange oldExchange, Exchange newExchange) { Exchange aggregateExchange = initializeAggregateExchange(oldExchange, newExchange); log.info(String.format(&quot;Pending messages: [%s] messages&quot;, aggregateExchange.getIn().getBody(SortedSet.class).size())); appendMessage(aggregateExchange, newExchange.getIn()); findAndEmitSequencedMessages(aggregateExchange); return aggregateExchange; } protected Exchange initializeAggregateExchange(Exchange oldExchange, Exchange newExchange) { Exchange aggregateExchange; if (oldExchange == null) { aggregateExchange = ExchangeHelper.copyExchangeAndSetCamelContext(newExchange, camelContext); SortedSet&lt;Message&gt; pendingMessages = new TreeSet&lt;&gt;(_messageComparator()); aggregateExchange.getIn().setBody(pendingMessages); aggregateExchange.setProperty(LAST_PROCESSED_INDEX, -1L); } else { aggregateExchange = oldExchange; } return aggregateExchange; } protected void appendMessage(Exchange aggregateExchange, Message message) { log.info(String.format(&quot;Adding message: index [%s], body [%s]&quot;, message.getHeader(sequenceIdHeaderName), message.getBody())); aggregateExchange.getIn().getBody(SortedSet.class).add(message); } protected void findAndEmitSequencedMessages(Exchange aggregateExchange) { SortedSet&lt;Message&gt; pendingMessages = aggregateExchange.getIn().getBody(SortedSet.class); Long lastProcessedIndex = aggregateExchange.getProperty(LAST_PROCESSED_INDEX, Long.class); Message currentMessage; Long currentMessageIndex; SortedSet&lt;Message&gt; messagesToBeEmitted = new TreeSet&lt;&gt;(_messageComparator()); do { currentMessage = pendingMessages.first(); currentMessageIndex = currentMessage.getHeader(sequenceIdHeaderName, Long.class); if (currentMessageIndex == lastProcessedIndex + 1) { messagesToBeEmitted.add(currentMessage); pendingMessages.remove(currentMessage); lastProcessedIndex = currentMessageIndex; } else { break; } } while (!pendingMessages.isEmpty()); if (!messagesToBeEmitted.isEmpty()) { log.info(String.format(&quot;Messages to be emitted: [%s] messages&quot;, messagesToBeEmitted.size())); aggregateExchange.setProperty(LAST_PROCESSED_INDEX, lastProcessedIndex); aggregateExchange.getIn().setBody(pendingMessages); Exchange exchangeToBeEmitted = ExchangeHelper.copyExchangeAndSetCamelContext(aggregateExchange, camelContext); exchangeToBeEmitted.getIn().setBody(messagesToBeEmitted); try { for (Processor processor : _aggregateProcessor().next()) { processor.process(exchangeToBeEmitted); } } catch (Exception e) { throw new RuntimeCamelException(e); } } }} Here’s a link to the full source for your perusal: [https://github.com/joshdreagan/camel-streaming-aggregation]. In this implementation, I was asked to do ordering aggregation of incoming messages. But as the messages came in, if the next sequential block was completed, the customer wanted those messages to be emitted at that time instead of waiting for the entire batch to complete. So, for example, if I got messages [1,3,5], those messages would be aggregated and stored in the aggregation repository. But then, when message [2] came in, messages [1,2,3] would be emitted/processed (while message [5] would remain in the repository). Finally, when message [4] came in, messages [4,5] would be emitted/processed. That’s about as custom as they come! Hopefully this helps highlight some of the power and flexibility of Camel. Like I said at the beginning of this post, your imagination is the limit (or rather your use case). Enjoy!","link":"/2018/08/30/camel_aggregation_strategies/"},{"title":"Camel CXFRS Contract First","text":"I’ve recently had a rash of customers who want to do contract driven REST development. That is, they want to define the contract for their service up front, and then generate their interfaces and models. And like all subjects, if I get asked about it enough times, I’ll write it down in a blog. So here goes… First, why would someone want to generate code from their contract? Well… we certainly don’t want to have to keep code and contract in sync manually. That would be extremely error prone (and would repeat the same mistakes we made in the early days of SOAP/WSDL). So that leaves two options: Either we generate the contract from the code (ie, code-first), or we generate the code from the contract (ie, contract-first). If we choose to go the code-first route, what we usually do is add some annotations (or add additional metadata via some language/library specific means), deploy the code to a running server, and then pull the contract by hitting (HTTP GET’ing) a special url. This seems all fine and good since it gives us a contract that is exposed for clients to consume. What more do we need right? As it turns out though, many organizations don’t do development this way. In fact, many of them will want to develop their contract well before implementation takes place. Often times, that contract is even developed by a separate team. So in these cases, we need to go contract-first. Which means that we have to figure out a way to generate the code… Now that we’re through all of that “intro” business, let’s talk about how one might actually accomplish this. There are many competing specs for defining REST contracts (ie, WADL, RAML, API Blueprint, …), but the most popular (and the one I’ll be targeting for this post) seems to be Swagger/OpenAPI. If you check out Swagger’s website, you’ll see that they’ve implemented some tooling to do this code generation. What’s more, the tooling is actually pretty pluggable and can be used to generate implementation/model files for many different languages. It doesn’t have the best documentation, and can be a bit buggy at times. But luckily it’s open source. So we can just read through the code to figure out how it works. And after all, code is really the best documentation… ;) Here’s a snippet of what the plugin configuration might look like: 1234567891011121314151617181920212223242526&lt;plugin&gt; &lt;groupId&gt;io.swagger&lt;/groupId&gt; &lt;artifactId&gt;swagger-codegen-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${swagger-codegen-maven-plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;generate-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;generate&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;inputSpec&gt;src/main/swagger/api.json&lt;/inputSpec&gt; &lt;language&gt;jaxrs-cxf&lt;/language&gt; &lt;generateSupportingFiles&gt;false&lt;/generateSupportingFiles&gt; &lt;modelPackage&gt;org.apache.camel.examples.model&lt;/modelPackage&gt; &lt;apiPackage&gt;org.apache.camel.examples.api&lt;/apiPackage&gt; &lt;output&gt;${project.build.directory}/generated-sources&lt;/output&gt; &lt;generateApiTests&gt;false&lt;/generateApiTests&gt; &lt;configOptions&gt; &lt;sourceFolder&gt;swagger&lt;/sourceFolder&gt; &lt;implFolder&gt;swagger&lt;/implFolder&gt; &lt;/configOptions&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; Once we have the plugin properly configured, it will bind to the generate-sources lifecycle phase of our Maven build. Which means that the code it outputs will be on our classpath, and all we need to do is create our implementation. Here’s a snippet of how you might do just that using Apache Camel (the most awesome framework available!): 1234567from(&quot;cxfrs:/camel?resourceClasses=org.apache.camel.examples.api.MyServiceApi&quot;) .routingSlip(simple(&quot;direct:${headers[operationName]}&quot;));from(&quot;direct:myServiceOperation&quot;) .log(&quot;This is where you should fill in your method implementations...&quot;); If you want to see what a full project looks like, I’ve created an example that uses the above tooling to generate the JAXRS/CXF annotated Java interfaces/model files from a real API JSON file that I downloaded from Swagger’s website. It then uses those generated files to create an implementation in Camel, and runs it on a Spring Boot container. Give it a look here: https://github.com/joshdreagan/camel-swagger-contract-first. So now that we know how to do contract-first REST development (and even have an example), we can just copy/paste any time we want to start a new project. But can we do better? I think so… As it turns out, Maven has a baked-in mechanism for creating templates to generate new projects. They’re called Maven Archetypes. Usually, you just create an archetype project as defined by the docs. The archetype project contains a fairly simple descriptor XML, and a bunch of Velocity templates for each of the files you want to be included in your generated project. This allows you to do simple property expansion (ie, groupId, artifactId, package, …), conditionals, and even some looping inside of your template files. Once you build/install the archetype, you can crank out new projects by simply executing a command like this: 1234567$ mvn archetype:generate -DarchetypeCatalog=local \\ -DarchetypeGroupId=org.apache.camel.archetypes \\ -DarchetypeArtifactId=my-custom-archetype \\ -DarchetypeVersion=1.0.0-SNAPSHOT \\ -DgroupId=com.mycompany \\ -DartifactId=myproject \\ -Dversion=1.0.0-SNAPSHOT However, if you looked at the example project I gave you above, you’d notice that some of the files need values that are parsed from the Swagger API doc. So Velocity templates alone won’t cut it… We could just leave those values blank (or put in some sort of placeholder), and then have the user fill them in after the project generation. But once again, we can do better… There is a little known (and horribly under-documented) feature that we can take advantage of to automate things. If you look under the “Advanced Usage” section of the archetype plugin docs, you will see a reference to a “Post-generation script” (toward the bottom of the page). Basically, you can just include a Groovy script named archetype-post-generate.groovy inside the src/main/resources/META-INF/ folder, and its contents will be executed during project generation (after the Velocity templates have been applied, and the files copied into their destinations). And because Groovy is super awesome and incredibly powerful, we can use it do pretty much anything we want. For instance, parsing up a JSON file and using its values to replace extra placeholders in our project files… Neat! Here’s an example archetype project to get you started: https://github.com/joshdreagan/camel-archetype-spring-boot-cxfrs-contract-first. Enjoy!","link":"/2018/03/02/camel_cxfrs_contract_first/"},{"title":"Connecting FeedHenry to Fuse REST Quickstart","text":"This guide demonstrates step-by-step how to connect a FeedHenry Hello World application to a JBoss Fuse REST quickstart backend running on Red Hat’s OpenShift Online. Here are a few assumptions before you begin: You've already completed the guide in my previous blog about creating a FeedHenry Hello World project You've already completed the guide in my previous blog about creating a Fuse REST quickstart project on OpenShift Once you’ve completed the prerequisite blog guides, connecting the two is a simple matter of modifying a few files. You can even use the browser based editor so you can do everything online. Log into your FeedHenry domain and find your previously created Hello World project. Step 1: Modify the Hello World Cloud AppFirst, you’ll modify the cloud app files. Click on the “Cloud App“ panel shown below. Next, click on the “Editor“ button in the column on the left. Edit the /package.json file and add a dependency for “xml2js“ version “~0.4.8“ to the end of the dependencies section. If you’d like, you can copy/paste the entire contents from the text below. 1234567891011121314151617181920212223242526272829303132333435{ &quot;name&quot;: &quot;fh-app&quot;, &quot;version&quot;: &quot;0.2.0&quot;, &quot;dependencies&quot;: { &quot;body-parser&quot;: &quot;~1.0.2&quot;, &quot;cors&quot;: &quot;~2.2.0&quot;, &quot;express&quot;: &quot;~4.0.0&quot;, &quot;fh-mbaas-api&quot;: &quot;~4.9.0&quot;, &quot;mocha&quot;: &quot;^2.1.0&quot;, &quot;request&quot;: &quot;~2.40.0&quot;, &quot;xml2js&quot;: &quot;~0.4.8&quot; }, &quot;devDependencies&quot;: { &quot;grunt-contrib-watch&quot;: &quot;latest&quot;, &quot;grunt-nodemon&quot;: &quot;0.2.0&quot;, &quot;grunt-concurrent&quot;: &quot;latest&quot;, &quot;supertest&quot;: &quot;0.8.2&quot;, &quot;should&quot;: &quot;2.1.1&quot;, &quot;proxyquire&quot;: &quot;0.5.3&quot;, &quot;grunt-shell&quot;: &quot;0.6.4&quot;, &quot;istanbul&quot;: &quot;0.2.7&quot;, &quot;grunt-env&quot;: &quot;~0.4.1&quot;, &quot;load-grunt-tasks&quot;: &quot;~0.4.0&quot;, &quot;time-grunt&quot;: &quot;~0.3.1&quot;, &quot;grunt-node-inspector&quot;: &quot;~0.1.5&quot;, &quot;grunt-open&quot;: &quot;~0.2.3&quot;, &quot;grunt-plato&quot;: &quot;~1.0.0&quot; }, &quot;main&quot;: &quot;application.js&quot;, &quot;scripts&quot;: { &quot;test&quot;: &quot;grunt test&quot;, &quot;debug&quot;: &quot;grunt serve&quot; }, &quot;license&quot;: &quot;mit&quot;} Next, edit the /lib/hello.js file and replace its contents with the text below. Make sure to replace the uri field in both the get and post methods with the URI to your Fuse REST quickstart that you created in the previous guide. 123456789101112131415161718192021222324252627282930313233343536373839var express = require('express');var bodyParser = require('body-parser');var cors = require('cors');var request = require('request');var xml2js = require('xml2js');function helloRoute() { var hello = new express.Router(); hello.use(cors()); hello.use(bodyParser()); hello.get('/', function(req, res) { console.log(new Date(), 'In hello route GET / req.query=', req.query); var id = req.query &amp;&amp; req.query.hello ? req.query.hello : '000'; request({uri: 'http://restchild-joshdreagan.rhcloud.com/cxf/crm/customerservice/customers/' + id, method: &quot;GET&quot;}, function (error, response, body) { xml2js.parseString(body, function (err, result) { var name = result.Customer.name[0]; res.json({msg: 'Hello ' + name}); }); }); }); hello.post('/', function(req, res) { console.log(new Date(), 'In hello route POST / req.body=', req.body); var id = req.body &amp;&amp; req.body.hello ? req.body.hello : '000'; request({uri: 'http://restchild-joshdreagan.rhcloud.com/cxf/crm/customerservice/customers/' + id, method: &quot;GET&quot;}, function (error, response, body) { xml2js.parseString(body, function (err, result) { var name = result.Customer.name[0]; res.json({msg: 'Hello ' + name}); }); }); }); return hello;}module.exports = helloRoute; Finally, edit the /README.md file and change the sample request body to “123“ instead of “world“. If you’d like, you can copy/paste the entire contents from the text below. This step is optional, but is nice to do since it will allow you to test directly from the “Docs“ page. 12345678910111213141516171819202122232425# FeedHenry Hello World MBaaS ServerThis is a blank 'hello world' FeedHenry MBaaS. Use it as a starting point for building your APIs.# Group Hello World API# hello [/hello]'Hello world' endpoint.## hello [POST]'Hello world' endpoint.+ Request (application/json) + Body { &quot;hello&quot;: &quot;123&quot; }+ Response 200 (application/json) + Body { &quot;msg&quot;: &quot;Hello world&quot; } Now, save all of the files and click on the “Deploy“ button in the column on the left. That should bring you to the “Deploy“ page. Click on the “Deploy Cloud App“ button. Deployment should take less than 1 minute. You will be shown a progress bar and status text box. When the deploy process is complete, you will see a green progress bar and no errors in the status text box. Once everything deploys successfully, you can click on the “Docs“ button in the column on the left. This will take you to the test page for your app. It should look like the picture below. Click on the “Try it!“ button to expand the request &amp; response text boxes. Click on the “Submit“ button to invoke the service. You should see a successful response like the one pictured below. Step 2: Modify The Hello World Cordova Light AppNow, you’ll modify the Cordova application files. Click on the “Cordova Light App“ panel shown below. Next, click on the “Editor“ button in the column on the left. Edit the /www/index.html file and replace the placeholder text for the input box with “Enter Your ID Here.“. If you’d like, you can copy/paste the entire contents from the text below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!DOCTYPE html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, user-scalable=no&quot;&gt; &lt;title&gt;Hello World&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;css/app.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;header class=&quot;header-footer-bg&quot;&gt; &lt;div class=&quot;center&quot;&gt; &lt;h3&gt;FeedHenry &lt;small&gt;QuickStart - HTML5&lt;/small&gt; &lt;/h3&gt; &lt;/div&gt; &lt;/header&gt; &lt;div id=&quot;count&quot; class=&quot;&quot;&gt; &lt;div id=&quot;formWrapper&quot;&gt; &lt;p id=&quot;description&quot;&gt;This is a basic Javascript App that can take in your name, send it to a cloud app and display the response.&lt;/p&gt; &lt;br&gt; &lt;div class=&quot;input-div&quot;&gt; &lt;input id=&quot;hello_to&quot; type=&quot;text&quot; class=&quot;input-text&quot; placeholder=&quot;Enter Your ID Here.&quot;/&gt; &lt;/div&gt; &lt;br&gt; &lt;button id=&quot;say_hello&quot; type=&quot;button&quot; class=&quot;say-hello-button&quot;&gt;Say Hello From The Cloud&lt;/button&gt; &lt;div id=&quot;cloudResponse&quot; class=&quot;cloudResponse&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;footer class=&quot;header-footer-bg&quot;&gt; &lt;div&gt; &lt;small class=&quot;right&quot;&gt; &lt;!-- v.&amp;nbsp; --&gt; &lt;/small&gt; &lt;/div&gt; &lt;/footer&gt; &lt;script src=&quot;feedhenry.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;hello.js&quot;&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; Don’t forget to save the file. The simulator on the right should automatically update. Don’t worry if it doesn’t. Sometimes it needs a page refresh. However, since the changes we made were purely cosmetic, it doesn’t really matter if you see the update. Enter “123“ into the input box on the simulator and click “Say Hello From The Cloud“. If everything was successful, you should see a response like the one pictured below. Now you’ve successfully wired your FeedHenry Hello World project to a backend Fuse REST quickstart running on OpenShift.","link":"/2015/05/08/connecting_feedhenry_to_fuse_rest_quickstart/"},{"title":"Correcting Malformed Data With Fuse+BPMS","text":"The ProblemIn many environments, it is common to ingest data as a batch process. This is usually accomplished by polling a directory (or FTP server) for files. The issue is that polling for files is a one-way communication. So if you get a file that you can’t parse (or one that is invalid for any other reason), you have no way of communicating that back to the original sender. How do you handle the error? You likely can’t just drop the data on the floor. That really only leaves one option and that is to correct the data and re-ingest it. The SolutionSo we need to manually correct the data. We could create our own state machine to walk through the steps for correction. But we’d need to create a task queue, handle locking/timeouts, and create some UIs to allow the users to pull up the data and fix it. Also, we’ll probably want to restrict access to some group (like “analyst”), and since the task will likely be long-running we’ll want to create a set of UIs that allow operators to view the current state. All of this requires a significant amount of effort to complete (and is quite difficult to get right). Or… we could just use BPMS which already does all of that for us. Take a look at the Malformed Data Fixer-Upper project on GitHub. Follow the instructions in the README.md file to get it up and running. This example is a bit contrived, but it illustrates the general pattern that you might follow. Basically, Fuse is used to poll for files and validate them. If they are found to be invalid, Fuse will invoke a jBPM process with a Human Task node. Once a human corrects the data, it will be passed back to Fuse to be validated/processed again. The interaction between Fuse and BPMS is done via SOAP/HTTP. So BPMS hosts a web service and Fuse hosts a callback web service. The following diagram shows the interaction in more detail (including what data is passed).","link":"/2015/10/30/correcting_data_with_fuse_and_bpms/"},{"title":"Custom Camel LoadBalancer With Infinispan","text":"Apache Camel is a pretty full-featured EIP implementation framework. It has several existing strategies for load-balancing right out of the box. Round Robin, Random, Sticky, Weighted Round Robin, Weighted Random,… the list goes on and on. But being that it’s a very well written and pluggable framework, it also gives you the ability to drop in your own custom strategies should you find that none of the existing ones meet your specific needs. So for this post, I created a custom Camel Load Balancer implementation utilizing an Infinispan cache to dynamically discover and load-balance between destination endpoints. The sample code for this post can be found at https://github.com/joshdreagan/infinispan-discovery Why?Why would I do such a thing? Well… there’s a few good reasons. First, all of the existing load-balancer strategies work on a static list. So if I know all of my endpoints ahead of time, no problem. I just code them into my Camel route. But what if my list of endpoints changes between environments? Maybe I could use properties. Well… only if the number of endpoints is static. Which brings me to the next reason… All of the existing load-balancer strategies are configured at startup. So what do I do if my list changes dynamically at runtime? Let’s say that I want to do service discovery and load-balance between the currently active/registered backend services. If you’re familiar with Camel, you might be thinking “Why not just use the Camel Fabric Component? It does dynamic load-balancing and service discovery. Problem solved right?”. If all of my services are running in containers that are managed by Fabric8, that is a viable solution. But what if I want to discover some endpoints that are running on JBoss EAP instances. Or what if I’m not running a Fabric8 ensemble at all? Finally, the most important reason… Because I can. :) Implementation DetailsCreating a custom Camel Load Balancer implementation is fairly straight forward. You just create a class and implement the LoadBalancer interface. There’s even a base class (LoadBalancerSupport) that you can extend that will take care of some of the boilerplate coding for you. You then just fill in the details of how it picks the next endpoint from its internal list. Pretty simple right? In my case, however, I’m not actually coming up with my own strategy for how to pick endpoints. I’m really just augmenting some existing strategy with a dynamic list of endpoints. So to be more specific, I’m not interested in implementing my own flavor of the Random, Round Robin, Sticky, … strategies. No need to reinvent that wheel. Instead, I just want to decorate those existing strategies and provide them with some additional capabilities. So I use the decorator pattern. That allows me to ignore all the tom-foolery of the load-balancing itself and concentrate on the portion that I really want. The dynamic service discovery. Here’s my custom load-balancer class (or at least the important parts): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150package org.apache.camel.processor.loadbalancer;// Import statements removed for brevity.public class JCacheLoadBalancer extends ServiceSupport implements LoadBalancer, CamelContextAware, InitializingBean { private final Logger log = LoggerFactory.getLogger(JCacheLoadBalancer.class); private static final LoadBalancer DEFAULT_DELEGATE = new RoundRobinLoadBalancer(); private Cache&lt;String, Set&lt;String&gt;&gt; registry; private String groupId; private LoadBalancer delegate; private UriPreProcessor uriPreProcessor; private Boolean throwExceptionIfEmpty; private Map&lt;String, Processor&gt; processorMap; private CacheEntryListenerConfiguration&lt;String, Set&lt;String&gt;&gt; registryListenerConfiguration; private CamelContext camelContext; // Getters and setters removed for brevity. @Override public void addProcessor(Processor processor) { delegate.addProcessor(processor); } @Override public void removeProcessor(Processor processor) { delegate.removeProcessor(processor); } @Override public List&lt;Processor&gt; getProcessors() { return delegate.getProcessors(); } @Override public boolean process(Exchange exchange, AsyncCallback callback) { if ((getProcessors() == null || getProcessors().isEmpty()) &amp;&amp; throwExceptionIfEmpty) { if (throwExceptionIfEmpty) { exchange.setException(new LoadBalancerUnavailableException(String.format(&quot;No URIs found for service '%s'.&quot;, groupId))); } callback.done(true); return true; } else { return delegate.process(exchange, callback); } } @Override public void process(Exchange exchange) throws Exception { if ((getProcessors() == null || getProcessors().isEmpty()) &amp;&amp; throwExceptionIfEmpty) { throw new LoadBalancerUnavailableException(String.format(&quot;No URIs found for service '%s'.&quot;, groupId)); } delegate.process(exchange); } @Override protected void doStart() throws Exception { if (delegate == DEFAULT_DELEGATE) { ((Service) delegate).start(); } registry.registerCacheEntryListener(registryListenerConfiguration); processUris(registry.get(groupId)); } @Override protected void doStop() throws Exception { registry.deregisterCacheEntryListener(registryListenerConfiguration); if (delegate == DEFAULT_DELEGATE) { ((Service) delegate).stop(); } } @Override public void afterPropertiesSet() throws Exception { Objects.requireNonNull(registry, &quot;The registry property must not be null.&quot;); Objects.requireNonNull(groupId, &quot;The groupId property must not be null.&quot;); Objects.requireNonNull(camelContext, &quot;The camelContext property must not be null.&quot;); if (delegate == null) { delegate = DEFAULT_DELEGATE; } if (throwExceptionIfEmpty == null) { throwExceptionIfEmpty = true; } processorMap = new HashMap&lt;&gt;(); registryListenerConfiguration = new MutableCacheEntryListenerConfiguration&lt;&gt;(new LookupCacheListenerFactory(), null, false, false); } private void processUris(Set&lt;String&gt; uris) throws Exception { if (uris == null) { uris = new HashSet&lt;&gt;(); } for (String uri : processorMap.keySet()) { if (!uris.contains(uri)) { log.info(String.format(&quot;Removing uri: %s&quot;, uri)); Processor processor = processorMap.remove(uri); removeProcessor(processor); if (processor instanceof Producer) { camelContext.removeEndpoint(((Producer) processor).getEndpoint()); } } } for (String uri : uris) { if (!processorMap.containsKey(uri)) { log.info(String.format(&quot;Adding uri: %s&quot;, uri)); String processedUri = uri; if (uriPreProcessor != null) { processedUri = uriPreProcessor.process(uri); } Processor processor = camelContext.getEndpoint(processedUri).createProducer(); processorMap.put(uri, processor); addProcessor(processor); } } } private class LookupCacheListener implements CacheEntryCreatedListener&lt;String, Set&lt;String&gt;&gt;, CacheEntryUpdatedListener&lt;String, Set&lt;String&gt;&gt;, CacheEntryRemovedListener&lt;String, Set&lt;String&gt;&gt;, CacheEntryExpiredListener&lt;String, Set&lt;String&gt;&gt;, Serializable { // All listener methods removed for brevity. They just delegate to the onEvent method anyway. public void onEvent(Iterable&lt;CacheEntryEvent&lt;? extends String, ? extends Set&lt;String&gt;&gt;&gt; events) throws CacheEntryListenerException { for (CacheEntryEvent&lt;? extends String, ? extends Set&lt;String&gt;&gt; event : events) { log.debug(String.format(&quot;Got a cache event: %s&quot;, event)); try { processUris(event.getValue()); } catch (Exception e) { throw new CacheEntryListenerException(e); } } } } private class LookupCacheListenerFactory implements Factory&lt;LookupCacheListener&gt; { @Override public LookupCacheListener create() { return new LookupCacheListener(); } }} You can see that it’s just delegating most of the methods (ie, the addProcessor, getProcessor, and removeProcessor methods) to whatever existing implementation that it’s decorating. The actual methods that do the load-balancing (ie, the process methods) do a little bit of work, but end up just delegating as well. So I didn’t actually have to do any algorithm work and I still get to use all the existing strategies. Pretty neat! In addition to a delegate LoadBalancer implementation, this class expects that you will give it a fully-configured jCache instance. In my example, I used Infinispan. But I could have just as easily used any other spec compliant implementation. Here’s my Infinispan configuration: 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;infinispan xmlns=&quot;...&quot;&gt; &lt;jgroups&gt; &lt;stack-file name=&quot;external-file&quot; path=&quot;default-configs/default-jgroups-tcp.xml&quot;/&gt; &lt;/jgroups&gt; &lt;cache-container default-cache=&quot;default&quot;&gt; &lt;local-cache name=&quot;registry-cache&quot;/&gt; &lt;transport cluster=&quot;registry-cluster&quot; stack=&quot;external-file&quot;/&gt; &lt;replicated-cache name=&quot;registry-cache&quot; mode=&quot;SYNC&quot; /&gt; &lt;/cache-container&gt;&lt;/infinispan&gt; Now let’s get to the part that’s actually doing some work. The LookupCacheListener class just implements the various CacheListener interfaces from the jCache API. If it gets any events on the cache entry containing our endpoints, it simply updates the delegate’s internal list of processors. So as services come and go they can register their URIs in the cache, our listener will be notified, and our list of available load-balancer endpoints will be updated. The final piece to discuss for this load-balancer implementation is the UriPreProcessor. This is an interface that I created to allow an implementation to customize the URI in some way before adding it to the list. The idea is that other services that are registering themselves might not know that they’re going to be invoked from a Camel endpoint. So they likely won’t add options like bridgeEndpoint=true to the URI. An implementation of this interface would allow you to add such options on their behalf. Here’s the interface itself: 123456package org.apache.camel.processor.loadbalancer;public interface UriPreProcessor { public String process(String uri) throws Exception;} And here’s a sample implementation that adds the options: 12345678910111213package org.jboss.poc.greeter.camel;// Import statements removed for brevity.public class GreeterServiceUriPreProcessor implements UriPreProcessor { @Override public String process(String uri) throws Exception { Map&lt;String, Object&gt; options = new HashMap&lt;&gt;(); options.put(&quot;bridgeEndpoint&quot;, true); return URISupport.appendParametersToURI(uri, options); }} Now all that’s left is to actually use it in my Camel routes. To do so, I declare it just like any other bean. Then I use the custom element in my loadBalance to ref it. Looks something like this: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;...&quot;&gt; &lt;bean id=&quot;cachingProvider&quot; class=&quot;javax.cache.Caching&quot; factory-method=&quot;getCachingProvider&quot;/&gt; &lt;bean id=&quot;cacheManager&quot; factory-bean=&quot;cachingProvider&quot; factory-method=&quot;getCacheManager&quot;&gt; &lt;constructor-arg value=&quot;META-INF/infinispan/infinispan-clustered.xml&quot;/&gt; &lt;constructor-arg&gt; &lt;bean class=&quot;org.springframework.util.ClassUtils&quot; factory-method=&quot;getDefaultClassLoader&quot;/&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id=&quot;registryCache&quot; factory-bean=&quot;cacheManager&quot; factory-method=&quot;getCache&quot;&gt; &lt;constructor-arg value=&quot;registry-cache&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;jCacheLoadBalancer&quot; class=&quot;org.apache.camel.processor.loadbalancer.JCacheLoadBalancer&quot; init-method=&quot;start&quot; destroy-method=&quot;stop&quot;&gt; &lt;property name=&quot;registry&quot; ref=&quot;registryCache&quot;/&gt; &lt;property name=&quot;groupId&quot; value=&quot;/services/soap-http/{http://poc.jboss.org/greeter}GreeterService&quot;/&gt; &lt;property name=&quot;delegate&quot; ref=&quot;randomLoadBalancer&quot;/&gt; &lt;property name=&quot;uriPreProcessor&quot; ref=&quot;greeterServiceUriPreProcessor&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;greeterServiceUriPreProcessor&quot; class=&quot;org.jboss.poc.greeter.camel.GreeterServiceUriPreProcessor&quot;/&gt; &lt;bean id=&quot;randomLoadBalancer&quot; class=&quot;org.apache.camel.processor.loadbalancer.RandomLoadBalancer&quot;/&gt; &lt;camelContext id=&quot;greeterGateway&quot; trace=&quot;false&quot; xmlns=&quot;http://camel.apache.org/schema/spring&quot;&gt; &lt;route id=&quot;proxyRoute&quot;&gt; &lt;from uri=&quot;jetty:http://localhost:9000/gateway/GreeterService?matchOnUriPrefix=true&amp;amp;bridgeEndpoint=true&quot;/&gt; &lt;onException&gt; &lt;exception&gt;org.apache.camel.processor.loadbalancer.LoadBalancerUnavailableException&lt;/exception&gt; &lt;handled&gt; &lt;constant&gt;true&lt;/constant&gt; &lt;/handled&gt; &lt;setHeader headerName=&quot;CamelHttpResponseCode&quot;&gt; &lt;constant&gt;404&lt;/constant&gt; &lt;/setHeader&gt; &lt;setBody&gt; &lt;constant&gt;NOT FOUND&lt;/constant&gt; &lt;/setBody&gt; &lt;/onException&gt; &lt;loadBalance&gt; &lt;custom ref=&quot;jCacheLoadBalancer&quot;/&gt; &lt;/loadBalance&gt; &lt;/route&gt; &lt;/camelContext&gt;&lt;/beans&gt; That’s it for the Camel side of things. Now let’s discuss how to get some services registered. In my example, I just created a simple JAX-WS service in JBoss WildFly. Here’s the code so you can see how simple it is: 1234567891011121314151617181920package org.jboss.poc.greeter.impl;// Import statements removed for brevity.@WebService(endpointInterface = &quot;org.jboss.poc.greeter.Greeter&quot;, serviceName = &quot;GreeterService&quot;, portName = &quot;GreeterServicePort&quot;)public class EnglishGreeter implements Greeter { @Override public String getGreeting(String name) { String greeting = null; try { greeting = String.format(&quot;Hello %s from %s!&quot;, name, InetAddress.getLocalHost().getHostAddress()); } catch (Exception e) { greeting = String.format(&quot;Hello %s! I was unable to find my IP :(...&quot;, name); } return greeting; }} For this service, I created a ServletContextListener to register/unregister it’s URI to/from the jCache. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package org.jboss.poc.greeter.impl;// Import statements removed for brevity.@WebListener()public class GreeterServiceRegistrar implements ServletContextListener { private static final String SERVICE_NAMESPACE = &quot;http://poc.jboss.org/greeter&quot;; private static final String SERVICE_NAME = &quot;GreeterService&quot;; private static final String REGISTRY_CACHE_NAME = &quot;registry-cache&quot;; private final Logger log = LoggerFactory.getLogger(GreeterServiceRegistrar.class); @Inject private DefaultCacheManager cacheManager; @Override public void contextInitialized(ServletContextEvent sce) { final String key = getServiceKey(); final String uri = getServiceURI(sce); if (cacheManager != null) { log.info(String.format(&quot;Registering service: {'%s': '%s'}&quot;, key, uri)); Cache&lt;String, Set&lt;String&gt;&gt; cache = cacheManager.getCache(REGISTRY_CACHE_NAME); Set&lt;String&gt; uris = cache.getOrDefault(key, new HashSet&lt;String&gt;()); uris.add(uri); cache.put(key, uris); } else { log.warn(String.format(&quot;Unable to register service: {'%s': '%s'}. CacheManager is null.&quot;, key, uri)); } } @Override public void contextDestroyed(ServletContextEvent sce) { final String key = getServiceKey(); final String uri = getServiceURI(sce); if (cacheManager != null) { log.info(String.format(&quot;Unregistering service: {'%s': '%s'}&quot;, key, uri)); Cache&lt;String, Set&lt;String&gt;&gt; cache = cacheManager.getCache(REGISTRY_CACHE_NAME); Set&lt;String&gt; uris = cache.getOrDefault(key, new HashSet&lt;String&gt;()); uris.remove(uri); cache.put(key, uris); } else { log.warn(String.format(&quot;Unable to unregister service: {'%s': '%s'}. CacheManager is null.&quot;, key, uri)); } } private String getServiceKey() { // Contents removed for brevity. Method forms and returns the cache key where we'll store our URIs. } private String getServiceURI(ServletContextEvent sce) { // Contents removed for brevity. Method grabs the current IP and port that the server is bound to and forms a URI. }} So now when my ServletContext is started, my JAX-WS service URI will be registered. And when it is stopped, my URI will be removed. Since I configured my Infinispan cache the same on both the JBoss WildFly and Camel sides, the local cache instances are connected and will receive events and updates. That’s it! If you want to give it a go, check out the full source code at https://github.com/joshdreagan/infinispan-discovery. Hopefully it’s useful…","link":"/2015/12/04/custom_camel_loadbalancer_with_infinispan/"},{"title":"Decommissioning JBoss A-MQ Brokers","text":"There are many reasons why someone might need to decommision a JBoss A-MQ broker. Perhaps you are taking a server down for maintenance. Maybe you’re trying to do an upgrade. Or maybe you’ve scaled up during a peak period and now need to scale back down. In any case, you likely don’t want the messages that are persisted in that store to be stuck until you bring things back online. And in the case that you don’t plan to bring things back online, you certainly don’t want them to be lost. So what do you do? One strategy that I see a lot of people employ is to stop all the producers, and then wait until all the messages get processed by the consumers. This works fine for a lot of customers. And if you’re currently doing it this way (and it’s working), don’t worry. You’re certainly not doing anything wrong. However, this requires a lot of coordination and planning. It requires coordination and planning because A-MQ (at the broker level) doesn’t really have the ability to stop the production of messages without also stopping the consumption of messages. This is due to the fact that the default configuration (which is what most people will use) only opens one transport connector (listener) that will service both producers and consumers. You can disconnect individual clients, but if they decide to reconnect there’s nothing that’s going to stop them. Most people control this on the application side. They just shut down all of their producer applications (or at least the initial ingress ones) and wait for the consumers to fully process the existing messages. Like I said, there’s nothing wrong with this approach if it’s working for you. But I’ve effectively shut my entire system down even if I’m only decommissioning a single broker. What if I can’t have that much downtime? Maybe I could get creative with my clustering and partition my load (ie, multiple network of brokers that are separated from eachother). Then I’d only affect a single partition of my cluster at a time. My producer clients could failover and reconnect to another partition during this downtime so it would seem as if I’m still operational. Then I could swap them back if desired when I’m done. Definitely a step in the right direction, but I’m still taking down a whole partition of brokers just to decommision one. Another option would be to open two separate transport connectors (listeners) and have producers connect to one and consumers to the other. I’ve complicated my client code a bit, but maybe that’s ok. It’s not too bad after all… And now I have the ability to shut down the producer transport connector separately from the consumer transport connector at a single broker level, thus ensuring that no more messages will be produced to my broker while still allowing them to be consumed. But what if I have a network of brokers setup? I’ll need to also disable my network connector so that messages don’t get forwarded to me. Ok… we’re getting better… One outstanding problem is that I now have to rely on the locally connected consumers to successfully process all of my messages. How long will I need to wait? How many consumers are even connected to my broker? This brings us to the final solution (and best in my opinion). I can take advantage of the fact that ActiveMQ is really just a very flexible set of libraries and I can create a “message drainer” application to purge my persistent store. What do I mean? Well, first I would create a simple Java app that will spin up an embedded broker. I can point that embedded broker at a KahaDB persistent store. Then I can start consuming messages from it (like I would from any broker) and send them off to another broker. And since my embedded broker is local (ie., inside my JVM), I can just connect to it using the VM transport. So I don’t even have to worry about remote clients. They can’t even see my broker. They will simply failover to another active broker as soon as I take mine down and have no knowledge that I’m even connected and draining the messages. Neat! Now I don’t have to worry about coordinating my applications, separating my transport connectors, bringing down brokers unnecessarily, … I can simply bring down the broker that I wish to decommission. Then just run my “message drainer” application, point it to the KahaDB of my downed broker, and give it the url of an active broker that I’d like to send my messages off to. Once I’ve finished draining the messages, I can get rid of my broker and it’s persistent store. Or if I was just doing maintenance, I can bring the broker back online and it will see its store with no messages. So no need to worry about dupicates. This solution is simple, requires no unnecessary downtime, and can be used in any situation from performing maintenance to down-scaling. Here’s some sample source code to get you started: https://github.com/joshdreagan/activemq-drainer. Enjoy!","link":"/2016/08/22/decommissioning_jboss_a-mq_brokers/"},{"title":"Faster File Consumption With Camel","text":"One of the most frequently requested use cases that I encounter out in the field is to ingest file-based data (batch or otherwise), and then validate, transform, process, store… it. Luckily, Apache Camel’s File and FTP components make this extremely easy. So much so, that it requires very little thought to get up and running. And if you’re consuming small numbers of larger batch files, perhaps the defaults are good enough and you don’t need to put much though into it. If, however, you’re consuming large numbers of smaller files and you want to get the highest possible performance, there are a few configurations that you might want to consider. When writing a file poller, one of the most commonly overlooked requirements is that you need some sort of mechanism to determine when the writer is done writing. If you grab a file before it’s complete, you’ll end up truncating it and ending up with garbage data. Unfortunately, how you would make that determination on one filesystem does not necessarily work on all filesystems. In addition, the different strategies will have different performance characteristics and usually end up being your biggest bottleneck (outside of the actual processing of the data). Luckily, Camel provides you with several strategies out-of-the-box and even allows you to create your own if none of them meet your needs. So how to choose… First, let’s cover the absolute fastest, most generic solution. If you control the process writing the file as well as the process reading the file, you can use a “temp file” strategy. That is, I can have my writer write a file with some sort of temporary name (ie, appending an “.inprogress” extension), and then atomically move it to its final name when the write is complete (ie, remove the “.inprogress” extension). I can then easily have my consumer filter out the temporary files and only consume files that are complete. Camel can do all of this work for you. So no need to panic over writing a bunch of extra code. Simply set the appropriate options on the producer (ie, tempFileName) and the consumer (ie, exclude or antExclude) and call it a day. :) Another similar solution is to use the “done file” strategy. In this strategy, you will write the file, and when it is complete you will write out an additional file with the same name and some “marker” extension (ie, “.done”). You will then instruct your consumer to only pick up files if it finds their corresponding “done” file. Again, Camel makes this a simple matter of configuration. Just set the doneFileName option on the producer and the doneFileName option on the consumer. To me, this seems a bit more clunky than the previous solution, but the end result is the same. Both of the previous strategies are extremely fast and will work on pretty much any filesystem. However, as previously stated, they require you to have control over the producer and consumer sides. So what if you only control the consumer? Well… you could use one of the readLock options. Unfortunately, most of the available readLock options are more concerned with making sure no other consumers pick up the same file than they are with making sure the writer is done writing. And since we have other ways to make sure other consumers don’t step on our toes, we’ll just concentrate on the options that help us with the latter issue. The most robust option that’s available out-of-the-box is the “changed” strategy. Basically, the way it works is that the consumer will grab a file and check its last-modified timestamp. It will then continue to check the last-modified timestamp (at the configured readLockCheckInterval) until it determines that the file has stopped changing (ie, the previous poll’s last-modified matches the current one). Once it has determined that the file is no longer changing, it will consume it and pass it to the rest of the route for processing. This strategy is an excellent option because it works pretty much anywhere (ie, local filesystem, FTP, SFTP, …), and is configurable enough to handle the case of “slow writers” (by tweaking/increasing the readLockCheckInterval option). And if you’re getting small numbers of larger files, it’s probably fast enough. But if you’re trying to consume large numbers of smaller files, you will quickly see the bottleneck… The current implementation will loop through each file and (for each file) check the timestamp. It will continue to loop and check the timestamp on that one file until it either detects that it has stopped changing, or it hits its timeout (configured via the readLockTimeout option). It will not move on to the next file until one of those conditions is satisified. Which means that, if I have lots of producers writing files, those files could all be stuck waiting to be consumed because of a single slow producer. In practice, I’ve actually seen this happen and it’s leads to a very bad situation where the polling itself starts to take too long (at the filesystem level and outside of the control of Camel) because of the sheer number of files in a directory. Once this starts to happen, it really just starts a snowball effect. So it’s difficult to recover from and usually requires manual intervention. So what do we do? Well… Luckily, Camel is awesome enough that it allows us to extend it whenever it’s out-of-the-box options don’t meet our needs. Suck on that competition! :) Specifically, in the previous scenario, we actually solved the problem by creating our own custom version of the “changed” strategy. Only, in our version, we didn’t pause and repeat checks on a single file. Instead, we looped through the files and (for each file) checked its stats. We then added those stats to a cache and moved on to the next file. On each subsequent poll, we would check the file’s stats against the cached ones to determine if it had stopped changing (for at least the readLockCheckInterval amount of time). This allowed us to continue processing any files that were ready without having to wait behind a single one that wasn’t. In practice, we were able to use this strategy to consume very large numbers of files with only a single server. Take a look at the sample source code if you’d like to give it a try: https://github.com/joshdreagan/camel-fastfile. Worth noting that this is a recreation of the original work (as best as I could remember) that I did with my awesome colleague Scott Van Camp whose awesome coding skills are only rivaled by his awesome beard growing skills. So he gets to share in the credit/blame… :)","link":"/2017/01/05/faster_file_consumption_with_camel/"},{"title":"Fuse Fabric Offline CI&#x2F;CD","text":"This is a follow-on to the excellent blog post that my colleague Christian Posta wrote on doing CI/CD with Fuse Fabric. In particular, this post will extend upon his write-up by showing how to migrate Fabric8 profiles and Maven artifacts into environments which are disconnected from the development environment. The sample code for this blog can be found at https://github.com/joshdreagan/fuse-fabric-promotion-build. This is just a forked version of Christian’s original sample code with the changes outlined below. Most of the steps and information found in Christian’s original blog post are still relevant. In fact, all of the steps leading up to the “Environment build“ and it’s use of the Fabric8 Maven Plugin’s “branch“ goal are exactly the same. The ProblemThe issue with the “branch“ goal is that it assumes connectivity to a running Fabric8 ensemble so that it can directly branch it’s internal Git repo and upload the profile zips. In addition, the Fabric8 machines will also need connectivity back to the Maven repository (ie, Nexus or Artifactory) where the artifacts are stored so that it can pull them all down and install them once the profile is applied to a container. This is not always feasible due to the fact that some companies will restrict connectivity between environments. For many companies, it is very common that there will be restricted access between development and production environments due to various security constraints. Furthermore, some production environments (ie, defense and classified networks) will have absolutely no access whatsoever to the development environment or even to the internet. Therefore, it will often be necessary to package and transfer all the necessary Fabric8 profile zips and Maven artifacts via a completely offline (and usually read-only) media like a CD-R. This will of course limit the amount of automation that you can achieve, but can easily be accomplished with a few extra steps. The SolutionThe simplest solution is to build a zip file containing everything we need so that we will require no outside access during installation. This can easily be accomplished using the Maven Assembly Plugin. Once we have the zip file, we can transfer it to the target environments, unzip it, import our Fabric8 profiles, and proceed with our installation. Since all the artifacts are included, installation/provisioning should proceed as smoothly as if we were connected to the internet. Modify the Environment buildThe first step is to modify the “Environment build“ by adding a Maven Assembly Plugin configuration. Add the following Maven Assembly Plugin configuration to your project: 12345678910&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;attach&gt;false&lt;/attach&gt; &lt;descriptors&gt; &lt;descriptor&gt;src/assembly/repository.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;/configuration&gt;&lt;/plugin&gt; Make sure to set the “attach“ configuration to false. Failure to do so could result in the zip file (which could be quite large) being inadvertently uploaded to your Maven repository. Next, you’ll need to define the assembly descriptor as follows: 1234567891011121314&lt;assembly xmlns=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.3&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.3 http://maven.apache.org/xsd/assembly-1.1.3.xsd&quot;&gt; &lt;id&gt;repository&lt;/id&gt; &lt;formats&gt; &lt;format&gt;zip&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;repositories&gt; &lt;repository&gt; &lt;includeMetadata&gt;true&lt;/includeMetadata&gt; &lt;/repository&gt; &lt;/repositories&gt;&lt;/assembly&gt; You can optionally add an “execution“ for the Maven Assembly Plugin and bind it to a lifecycle. I prefer to invoke it directly as follows: 1mvn assembly:single The output should be a zip file containing an offline Maven repository with all of the Fabric8 profile zips as well as the required Maven dependencies. You can now download/transfer that zip file via whatever means you have available to your target environment. Once you’ve transferred the zip file, you simply unzip it into the “~/.m2/repository” folder on each of the Fabric ensemble machines. Make sure to place it in the user home of the user that runs the Fuse instances. You can then import the Fabric8 profile zips with the following Karaf CLI commands: 12fabric:profile-import --version 1.1 mvn:com.redhat.demo.promotion/eip/2.0/zip/profilefabric:profile-import --version 1.1 mvn:com.redhat.demo.promotion/rest/1.5/zip/profile Since the profile zips now exist in the “~/.m2/repository” folder, they will be discovered and installed by the Fabric Maven Proxy without needing to go online. Once you apply the profiles to a container (or upgrade a container to the new version), the provisioned container instances will reach back to the Fabric ensemble to retrieve the necessary Maven artifacts. Similarly to the profile zips, they will be discovered and installed by the Fabric Maven Proxy from its “~/.m2/repository” folder and will be transferred over without ever needing to go online.","link":"/2015/08/04/fuse_fabric_offline_ci__cd/"},{"title":"Dropping Dups With Camel","text":"In my last post, I talked about a couple of strategies for setting up disaster recovery for Apache Artemis brokers. In hindsight though, I thought that the subject of the Idempotent Camel Consumer could have used a little more discussion. In that blog post, I mirrored data between two Artemis brokers using diverts and bridges, and then used an idempotent Camel consumer to prevent duplicate processing. In the code example I provided, I had configured the Camel consumer to use a relational DB as its IdempotentRepository implementation. Specifically, it was this part of the diagram that we’re talking about: The feedback I got was that people didn’t want to involve yet another component such as a relational DB into their messaging architecture. But the choice of a DB was really just an implementation detail. And not a terribly important detail at that. In fact, I mentioned that there were several other available implementations that could be swapped in with no change to the architecture or code. Just a simple modification of the bean wiring. It even gives me the option to plug in whatever idempotent repository implementation I’d like. There are several to choose from. So lets talk a little bit about the different available options, and how this all works in general. At the time of this writing, there are well over 10 available options. Unfortunately, not all of which are listed on the main EIP documentation page. No matter which one you choose, the behavior is the same. When a message is passing through your route, its key is checked against the messageIdRepository that you’ve configured. You get to pick what the key is. Technically, the interface allows for any object to be used as a key, but all of the implementations I’m aware of use a String. Which is the most common choice anyways… If that key already exists, the message is skipped (or optionally processed, but marked with a “CamelDuplicateMessage” header). If the key did not exist, the message will be processed and its key added to the repository so that, next time, it will be skipped (or marked). Of course there’s a bit more to it with exception handling and all, but the end result is that you never process the same message more than once. No matter how many times it is sent to you. This is a super important, but often overlooked feature that most clients really should implement. It’s currently the best solution that we have to the two generals problem in computer science. Some of the implementations are persistent (ie, FileIdempotentRepository, JdbcIdempotentRepository, KafkaIdempotentRepository, …). Some are not (ie, MemoryIdempotentRepository). Some give you the option (ie, InfinispanIdempotentRepository, HazelcastIdempotentRepository, …). You can pick the right one based on your requirements for speed vs ability to recover in the case of an application restart. Now that we’ve covered the basics, let’s get back to the original Artemis DR use case… Given that we wanted persistence, but no additional components (ie, DBs, data grids, …), my suggestion was the simple FileIdempotentRepository. It basically just uses a flat file to store key entries. Each entry being a single line within the file. So no extra components required. Just a filesystem. Which I’m going to go ahead and guess that you already had. Unfortunately, upon further inspection of the code, I saw some possible issues that might arise. First, it looks like it does handle concurrency, but only within a single JVM. While this is not an issue for the specific case we’re talking about (because each JVM would have its own store), it might be something to consider for others. The bigger issue though, is around performance. During normal operation, when a key is added to the FileIdempotentRepository the implementation will add it to an internal cache, and then append it to the persistent file store. If the application restarts, it will load its cache from that persistent file, and then business as usual. However, once the store reaches capacity, it will add to its internal cache, and then re-write the whole cache to the file store (squashing the previous file and its entries). So what you will see is that performance seems great until you hit your max file store size. Then it will tank (depending, of course, on how many entries you keep)… Not ideal. At this point, I could just recommend that you use one of the data grid/cache based implementations (of which there are several). They will all handle concurrency, can do persistence, and will perform great. But I’m stuck at home, and had a bit of free time. So I figured, why not crank out a new, improved file based implementation? I call my implementation the DirectoryIdempotentRepository. Basically, instead of writing a single file of idempotent keys, I just use atomic file operations to write keys as individual files within a directory. That way, I don’t have to worry about concurrent access (even across JVMs) as every operation is atomic. And no need to worry about file-locking since each piece of data is represented as a separate file. Furthermore, I got rid of the internal cache since I can just do a Files.exists(Path path, LinkOption... options) to see if my repo contains a key. Neat! Here’s a link to the full source: https://github.com/joshdreagan/camel-directory-idrepo. So now I have a fast, concurrency safe, simple implementation that requires no extra components. Just a filesystem. I still have to synchronize it between DCs though. But that can be done using the built-in mechanism that my filesystem provides (like Ceph or Gluster replication), or by using a “decorator” implementation that publishes the repository commands to a mirrored topic like I did in my previous post. This all got me thinking though… It’d be really nice if I could just use some native feature of Artemis to persist my idempotent repository. Then, I wouldn’t have to include any extra components, and I wouldn’t have to worry about synchronizing my data across DCs. All data (messages and idempotent keys) would be handled via a single persistence mechanism, and all replication would use the same divert/bridge features and follow the same pattern as my normal message queues. The problem is that, with a typical queue, messages are only removed via expiration or consumption. And, in the case of consumption, they can only be consumed once. That doesn’t really fit our requirements. But what about topics? Well, with a typical topic, messages can indeed be consumed by multiple consumers. But only if those consumers are active when the message was produced, or if they registered as a durable subscriber before the message was produced. And only once for each consumer. So that’s no good either. Ideally, we want a bounded fifo queue (so it won’t grow endlessly) that supports message expiry (so we can discard old messages), and lets multiple consumers read through both existing messages as well as receive new messages. Luckily, Artemis has a few tricks up its sleeve. There’s a really cool new feature called a “ring queue”. It operates just like a bounded fifo queue. You set a maximum size, and it will continue to append messages as they are received. Once it reaches its bound, it will keep accepting new messages, dropping off the oldest ones to make room. And like all destination types, it supports message expiry. So that gets us part of the way there. But if you consume a message from a ring queue, it’s gone. And no one else will get it. So that doesn’t really do us much good if we’re trying to use it for persistence. That is, if our application comes back online and tries to rebuild its state, our messages will have already been consumed and we will have no data. There is another feature, however, that was intended to be used with “last value queues”. It’s called the “non-destructive consumer”. Basically, it allows a consumer to consume a message from a queue, without Artemis removing said message. So that means that, if I have multiple consumers, they will all receive a copy of every message. If they restart, they will again receive a copy of every message (starting from the beginning of the queue). Perfect! So now we have everything we need to build an IdempotentRepository implementation that uses an Artemis ring queue as its persistence mechanism. On startup, we can read through the messages on the queue (in a non-destructive manner) and build an internal cache of keys. As new keys are added, we can simply update our internal cache, and publish a command message to the queue. If we restart, all of our data will still be there and we can rebuild the cache by simply re-reading through the queue. In addition, any additional applications or instances will receive the same data both at startup, and as they are added. And that’s exactly what I did… Take a look at the source if you want to see how it all works: https://github.com/joshdreagan/camel-artemis-idrepo. But how does this help me really? Well, now I have an IdempotentRepository implementation based entirely on Artemis for persistence. No external components required. Also, since my keys are being stored on an Artemis topic, I can simplify my previous DR architecture and remove the extra decorator I had created to publish and replicate my repository command messages to an Artemis topic, as well as the piece that processed said command messages. In the end, my simplified architecture looks like this: Much better! :)","link":"/2020/03/20/dropping_dups_with_camel/"},{"title":"Getting Started - First Fuse Project","text":"This guide demonstrates step-by-step how to create a sample Camel project in JBoss Developer Studio (JBDS) using the Maven archetypes that come out of the box. Specifically, we’ll be creating a Spring based file poller project. We’ll then deploy the project to a JBoss Fuse server and verify that it is running properly. Here are a few assumptions before you begin: You already have a supported JDK installed (ie, Oracle’s JDK version 6 or 7) You already have Maven version 3.x installed You already have JDBS installed (tested with version 7.1.1, but should work with newer) You already have the “JBoss Integration and SOA Development” tools for JBDS installed You already have JBoss Fuse version 6.1 installed After you’ve met all the prerequisites, you can proceed with the guide. Step 1: Create A New Fuse ProjectStart off by creating a new project. Click File-&gt;New-&gt;Project… to start the “New Project” wizard. Select “Fuse Project“ as the project type and click “Next &gt;“. Specify your preferred project location (or let it use the default) and click “Next &gt;“. Select the appropriate Maven archetype from the list. The one we use for this example is “org.apache.camel.archetypes:camel-archetype-spring:2.12.0.redhat-610379“. Enter your preferred Group Id, Artifact Id, Version, &amp; Package. When done, click “Finish“. Your new project should be created. If this is the first time, it can take a few minutes as JBDS downloads all the required Maven dependencies. Step 2: Run The Project LocallyTo run the project locally, right click on the Camel Context and select Run As-&gt;Local Camel Context (without tests). This will run the project using the Camel Maven Plugin which uses a simple embedded, containerless runtime. The project contains some sample files in the “src/data“ directory that will be picked up immediately once the Camel Context has successfully started. You should see output similar to the one below in the “Console“ tab. When you’re satisfied that everything is running properly, you can shut down the local runtime by clicking the clicking the icon in the “Console“ tab. Step 3: Deploy Project To JBoss FuseIf you don’t have one already, create a new server configuration in JBDS. Click on the “Servers“ tab. If you don’t have any server configurations, you will see a link like the image below. Click on the link to start the “New Server“ wizard. Select “JBoss Fuse 6.1 Server“ and click “Next &gt;“. Click the “Browse“ button and navigate to the root directory of your JBoss Fuse installation. Click “Next &gt;“ when done. Enter the appropriate details/credentials for your JBoss Fuse server and click “Finish“. You should now have a new server configuration as shown below. Next, start the server. You can do so by clicking the icon as shown below. You may get a warning about SSH keys like the one shown below. If so, just click “Yes“ to accept the new key. If the server starts successfully, you should see the JBoss Fuse shell shown below. Next we’ll build the project and deploy it to the running server. To build the project, right-click at the project root and select Run As-&gt;Maven install. You will see the output of the Maven build in the “Console“ tab. If there were no errors, you should the a BUILD SUCCESS message. Now select the “Shell“ tab again to switch back to the JBoss Fuse console. Enter the following command to deploy your project (make sure to substitute the Maven Group Id, Artifact Id, &amp; Version for the ones you specified in Step 1): 1osgi:install -s mvn:com.mycompany/spring-camel/1.0.0-SNAPSHOT Your bundle should install and give you a Bundle ID. This Bundle ID is the identifier you will use to interact with that bundle in the future (ie, to uninstall, stop, start, …). You can also type the following command to verify that there are no failures: 1osgi:list You should see your newly installed bundle as shown below. Finally, open up a file browser (or use a terminal) and copy the sample files from “camel-spring/src/data“ to the “src/data“ directory inside the JBoss Fuse installation folder. Now, switch back to the “Shell“ tab and type the following command in the JBoss Fuse console: 1log:display You should see the log statements printed toward the bottom of the log. At this point, you have successfully created and deployed a Spring based file poller project to JBoss Fuse.","link":"/2015/04/09/getting_started__first_fuse_project/"},{"title":"Getting Started - FeedHenry Hello World","text":"This guide demonstrates step-by-step how to create and run your first FeedHenry Hello World project. Step 1: Create A New FeedHenry ProjectWhen you log into your FeedHenry domain, you should end up at a landing page like the one pictured below. Click on the “Projects“ panel to go to the projects page. You should see all of your existing projects (if you have any). Click on the “New Project“ button to create a new project. You should see the “Hello World Project“ template toward the top of the list. If you don’t, use the search box to find it. Click the “Choose“ button to the right to select the template. Fill in the project name and click the “Create“ button. Your project should begin the creation process. This can take a few minutes and you will be presented with a progress bar and status text box. When your project has been successfully created, you will see the following successful status messages. Your project is now created and ready to use. Step 2: Explore Your ProjectNow that you have a project, let’s explore the structure a bit. You should see a project landing page similar to the one below. Click on the “Cloud App“ panel to explore the backend Hello World cloud app. You should see the “Details“ page for the cloud app. From here you can stop/start the cloud app as well as see other various bits of information such as the host it’s currently running on and which environment it’s running in. On the left column, click on the “Editor“ button. You should now see the directory structure of the source code for the cloud app. Open up the “/lib/hello.js“ file to view the source that actually does the work for this Hello World project. While you’re at it, click around on some of the other files and take a look at their contents to get an idea of how everything is glued together. Now go back to the project’s landing page and click on the “Cordova Light App“ panel to explore the mobile app. You should see the “Details“ page for the mobile app. From there, you can test the mobile app directly from the web browser. If you’d like to test directly from your phone, you can go to the “Export“ page and export/download a copy of the app for your desired platform. Step 3: Test Your ProjectNow that you have a project, and you’ve explored the structure a bit, let’s run a quick test using the browser based device simulator. Enter your name into the input box and click “Say Hello From The Cloud“. You should see results similar to those pictured below. That’s it! You’ve successfully created and tested a Hello World app in FeedHenry.","link":"/2015/05/07/getting_started__feedhenry_hello_world/"},{"title":"Getting Started - Fuse REST Quickstart on OpenShift","text":"This guide demonstrates step-by-step how to create a Fuse cluster and deploy one of the quickstart applications using Red Hat’s OpenShift Online infrastructure. The first thing you’ll do is open up a browser and navigate to OpenShift Online. If you don’t already have an account, you can sign up for a free one. However, the free account is restricted to 3 small gears which will not run JBoss Fuse instances. So you will need to upgrade to a paid account to complete this guide. Step 1: Create A Domain/NamespaceAfter you’ve created an OpenShift Online account and logged in, you will end up at the landing page. Before creating any apps, you’ll need to create a domain/namespace. If you already have a domain/namespace, and you’d like to re-use it, you can skip this step. This is a unique identifier that allows you to group applications. Click on the “Settings“ tab at the top of the page. Enter your desired domain/namespace and click the “Create“ button. Now you’re ready to create your JBoss Fuse cluster. Step 2: Create A JBoss Fuse ClusterTo create a JBoss Fuse cluster, first navigate to the applications page by clicking the “Applications“ button at the top of the screen. If you don’t already have any applications, you will see a link to “Create your first application now“ like in the picture below. Click the link and it will begin the “Create Application“ workflow. On the first page, you will see a list of available cartridges. You’ll want to find and click on the “JBoss Fuse 6.1“ cartridge pictured below. If you can’t find it, you can use the search box. Next, you’ll configure the cartridge. Fill in the “Public URL“ field with the desired application name. You’ll notice that it automatically appends your domain/namespace that you created in Step 1. Select the desired “Gear Size“. You must use at least a medium sized gear to run the JBoss Fuse cartridge. When you’re satisfied with your settings, click the “Create Application“ button. Be patient, as it might take a minute or so to finish. Do not navigate away (or refresh) the page! Once your application has been successfully created, you will be presented with an informational screen like the one shown below. Make sure to capture this information as you will need it later and there is no way to get it once you’ve left this page! Step 3: Provision A Child ContainerNow that you have a JBoss Fuse cluster, you can provision a child container with the actual quickstart deployed. Click on the “Applications“ tab at the top of the page, and you should see your newly created JBoss Fuse application. Click on that application, and it should open up the JBoss Fuse Management Console. You will need to login using the credentials that were displayed for you at the end of Step 2. After you login, you should see the JBoss Fuse Management Console’s landing page. Click on the “Runtime“ tab to view the list of servers that are currently part of this cluster. At the moment, there should be only 1 and it is the Fabric Registry server (indicated by the little cloud icon). Click on the “Create“ button toward the top right of the page to begin the “Create Container“ workflow. Fill in the desired “Container Name“ and “Gear Size“ making sure to use at least a medium or larger. You can enter your OpenShift Online credentials and click the “Login to OpenShift“ button to have it fill in the “OpenShift Domain“ field. Select the “rest“ profile under the “Example / Quickstarts“ folder. When done, click the “Create And Start Container“ button. Be patient, as it might take a minute or so to finish. Do not navigate away (or refresh) the page! Once the container has been created, you will be redirected back to the “Containers“ page. You will notice that there are now 2. One is the Fabric Registry server, and one is hosting your application. Click on the “APIs“ tab to view the base URI of the REST service you’ve just deployed. It will be in the “Location“ column. Step 4: Test The ApplicationOpen a browser and navigate to the URI that you discovered in Step 3, appending /customerservice/customers/123. If successful, you should see some XML text come back. If all went well, you’ve now got a running JBoss Fuse cluster with the REST quickstart deployed to a child instance.","link":"/2015/05/07/getting_started__fuse_rest_quickstart_on_openshift/"},{"title":"HA Deployments With Fuse","text":"When out and about, I often get the question: “How do I setup HA (high availability) with Red Hat’s JBoss Fuse?”. People ask this question and they expect a simple, straightforward answer. And why shouldn’t they? The question is simple enough right? I could ask the same question about something like Tomcat and get a well documented answer involving little more than a loadbalancer. Unfortunately, the answer for Fuse is a bit more involved and usually starts with the annoying response of “it depends”. So let’s expand on that a bit… Considering my previous example of Tomcat (which is just a Servlet container), what protocol does it speak? Easy! It only talks 1 protocol… HTTP. But what protocol does Fuse speak? Well… since it’s an integration framework, the answer is several. It may be consuming from a REST service and placing the contents in files. Or maybe it’s accepting HL7 messages over TCP and dumping them onto JMS queues. And the way that you’d make a REST service HA is very different from the way that you’d make a file consumer HA (which is very different from the way you’d make a JMS broker HA, …). So you can see that while the answer of “it depends” might be a bit annoying, it is actually the most accurate answer I could give. As of this writing, the Camel Components page lists 240 different components. And there’s no way I’m going to cover all of those in a blog post. So lets just focus on the one’s I run into most often. HTTPMany of the available Camel components speak HTTP. Among those would be CXFRS for REST services, CXF for SOAP services, and Jetty or Servlet for low level HTTP services. And because HTTP is so prolific, it is probably the most easily understood HA scenario we’ll cover. With HTTP, there are two modes that we need to talk about (stateless, &amp; stateful). Of the two, stateless is the most common and definitely the most recommended approach. It scales well, requires no coordination in a cluster, and is extremely easy to set up. You simply run however many instances you’d like of your service. The instances don’t have to know about eachother, and can be co-located or can be spread across datacenters. Once you have your services stood up, you simply place a loadbalancer (ie, Apache HTTPd, NGINX, HAProxy, …) in front of them. There are even strategies that can be used to make the loadbalancers themselves HA (ie, hosting multiple instances with their own class A DNS records). So if any instance of your service goes down, the loadbalancer will simply redirect traffic to one of the other available instances. The client will send in his next request and have no idea that he isn’t talking to the same instance. Honestly, this topic has been covered so much that I don’t need to go into great detail about it. But here’s a very generic picture: Stateful HTTP apps have fallen out of favor over the last decade or so. Which is a good thing in my opinion! They usually require some sort of session replication and/or coordination among the cluster. So now all of your instances need to know about eachother (limiting our ability to scale). And every time an object/value is placed in the session, it must be replicated to the other members (causing quite a bit of overhead that compounds as the cluster grows). We can mitigate some of these problems by being creative with our architecture. For instance, instead of having every member in the cluster connected in a mesh configuration we can split our servers into multiple meshes and do “sticky” loadbalancing to them. But it’s all a lot of hassle that you shouldn’t deal with if you can find a way to make your apps stateless instead. Once again, this is a topic that has been covered many times on the internet. The only thing specific to Fuse (and the only thing I’ll elaborate on in this blog) is setting up session replication for the Karaf container. For the most part, if you’re running Camel on any JavaEE app server (and tying to its Servlet container), it will have its own mechanism for session replication. Most of the time, this is completely hidden away from you, and you get it for “free” just by setting up your servers in a cluster configuration. For instance, JBoss EAP uses an internal Infinispan cache to store its session data. If you run your EAP instances in “domain” mode, they will automatically be clustered and will replicate sessions accordingly. While you can override the cache settings and tweak them to fit your needs, you usually don’t have to mess with it. However, if you are running on a Karaf container, you’ll have to do a bit more of the setup yourself. This is because Karaf doesn’t assume that you’re using sessions, or even that you’re using Servlets. And if you do decide to use Servlets, it doesn’t assume which Servlet container you’ll use (ie, Tomcat, Jetty, …). So when you use any of the HTTP based components that I listed above on Karaf, they will (by default) fire up an embedded Jetty container. Luckily, Jetty is pluggable enough that it allows you to swap out its session management implementation. So you could, for instance, setup and configure Jetty to use an Infinispan cluster. Take a look at the Jetty Docs for more details. In any case, the mechanism by which a session is handled is transparent to your application. So it’s really more of a configuration detail. And just to stay consistent, here’s another generic picture: HL7/MLLPHL7/MLLP is a TCP based protocol for the healthcare industry. Digging in a bit more, HL7 (Health Level Seven) is the definition of the format of the message (which can be text or XML based), and MLLP (Minimal Lower Layer Protocol) just defines a couple of bytes that wrap the message so we know where to start/stop when reading it in. Support for HL7v2 is provided via the HL7 component in conjunction with a TCP transport component for doing the actual socket handling (ie, Mina, or Netty). There is some work being done on an MLLP component that will make it a bit simpler to work with, but as of this writing it’s still a bit early. All of that said, regardless of the transport component that you choose, or whether you’re working with HL7v2 (text) or HL7v3 (XML), the interaction is stateless. That is to say that a client sends in a request message and synchronously receives an “ack/nack” response. That is the entire transaction. Any other interaction is a separate message/ack and is handled independently. So no server-side coordination is required by the message acceptors (see “Note“ below). And because no server-side coordination is required, you can just use any available TCP loadbalancer (similar to what we did with stateless HTTP above). Note: You may have a requirement to resequence the messages and process them in order. In which case, you can take a look at my previous blog: Ordered Messaging With ActiveMQ &amp; Camel. Here’s a sample NGINX loadbalancer configuration that I used for a recent engagement: 12345678910111213events { worker_connections 1024;}stream { server { listen 7000; proxy_pass tcp_backend; } upstream tcp_backend { server instance-1.local:7000; server instance-2.local:7000; }} And here’s a (really simple) sample architecture: File/FTPWhen consuming files using the File or FTP components, there are a couple of different strategies that you can use for an HA setup: active/passive, and active/active. In an active/passive configuration, you will have a single (master) instance polling for files. All of the other instances (slaves) will be waiting on some kind of lock. The slave instances will not begin polling for files until they detect that the master is no longer alive and well. In this way, we make sure that multiple JVMs running the same file poller config aren’t stepping on eachother’s toes while trying to consume the files. So how do we setup this active/passive coordination? If you’re using Fabric8 v1.x to cluster your Karaf instances, you can just use the Master component. It exploits the fact that a Fabric cluster uses a ZooKeeper ensemble internally and uses it as a locking mechanism. Nice and simple! But what if you’re not running in a Fabric cluster? You could stand up your own ZooKeeper ensemble… But that adds a bit of overhead that you might not be ok with if you’re not using it for anything else. Does that mean that you’re out of luck? Heck no! Camel rocks! We can just create our own custom RoutePolicy to do the same thing. Here’s an example that I threw together for a customer recently: https://github.com/joshdreagan/camel-singleton-policy. Often times, you won’t need the highest possible performance when polling for files. That’s because the most common use case is that I receive a few files for batch processing maybe once a day. So I can probably handle the actual polling/processing on a single instance. And in that case, the active/passive configuration would be perfectly fine. But what if I’m not receiving batch files once a day? What if I’m processing satellite perturbation data (TONS of tiny files) coming in 24/7 in a neverending stream? Maybe now I want to take advantage of my entire cluster to poll/process in an active/active configuration. That way I can scale it up… Luckily, Camel makes this extremely easy. Because, again, Camel rocks! If you look at the available options for the file component, you’ll notice that it basically has an Idempotent Consumer pattern baked right in (take a look at the inProgressRepository option). What’s more, it’s extremely flexible. It just needs any implementation of org.apache.camel.spi.IdempotentRepository. And there are already several implementations canned and ready to go. So you can use anything from Infinispan to a relational DB to coordinate your consumers. Here’s some sample code that uses Infinispan: https://github.com/joshdreagan/clustered-file-consumer. Feel free to plagiarize! JMS (ActiveMQ)JMS (ActiveMQ) is definitely the most difficult HA scenario that I’ll cover. Which is why I procrastinated and saved it until the end. Well… sort of… When talking about JMS on Fuse, you have to specify whether you mean from a client’s perspective, or from the broker’s perspective. HA from the client’s perspective is actually quite simple. You just use the Failover transport when creating your javax.jms.ConnectionFactory and the failover is handled for you. If a broker goes down, the client libraries will transparently reconnect to the next broker (whether it’s a slave, or another master) and keep on chugging with little more than a blip in performance. But let’s spend a little time and talk about the more complex case of making the broker itself HA. When we say “make the broker HA”, what we really mean is “make the in-flight data that the broker is storing HA”. Because of the nature of messaging and it’s typical use case/requirements, this almost always ends up being a trade-off for performance vs reliability. So first let’s cover the easiest, best performing solution. In a Master/Slave (active/passive) architecture, two or more brokers point to the same physical data store (typically KahaDB). Because the store can only be written to by one instance at a time, we must use some form of locking (similar to what we talked about in the File/FTP section). The lock implementation that ActiveMQ uses is pluggable. So you can specify your own custom ones. But there are defaults for each of the persistence adapters and I rarely see customers override them. Let me give a concrete example… When setting up ActiveMQ as a master/slave pair (non-Fabric managed) and using KahaDB, you would likely place the KahaDB storage directory on a shared filesystem (ie, NFSv4). Unless you customized the configuration, ActiveMQ would default to using an actual “lock” file. When the instances came up, they would both attempt to acquire a filesystem-level lock on that file. Whoever got there first would become master, would open up the KahaDB for read/write, and would open up any listeners to begin accepting client connections. Any other instances would fail to get the lock and would begin try-polling until they got it. And until then, they would not read/write the KahaDB, or accept client connections. So they’re passive… This is the simplest setup, but does have some caveats. First, we’ll need to setup a shared filesystem that both instances can see. This will likely be something like an NFSv4 share. And because we need an actual filesystem-level lock, we must use a filesystem that supports them (which is why I used the example of NFSv4 above and not NFSv3). You’ll probably also want to make the storage that hosts your NFS shares HA as well. So you’ll likely use a SAN or some hardware appliance that provides this functionality. If you do so, make sure that any data duplication/backup that occurs is fully synchronous and that filesystem-level locks are preserved during a failover! Looking at you EMC… Next, because of the shared storage, high throughput, and file locking requirements, the master/slave instances must live in the same datacenter. I’ve seen many clients try to skirt this requirement and it always ends badly. However, if set up correctly this provides nearly immediate failover of in-flight messages within a datacenter as well as high message throughput. So if this satisfies your requirements, stop here. So what if I have a requirement for HA across datacenters? Ok… let’s negotiate a little more. The simplest &amp; fastest solution is to use the master/slave setup outlined above, but also have a warm site setup in another DC. If you experience a full DC outage, you can manually migrate the storage hosting your KahaDB to the backup DC, configure an ActiveMQ instance to point to it, and bring it online. It doesn’t care where the KahaDB came from, or if it was previously owned by another instance. It will simply read in any messages that are currently stored and begin processing/delivering them to consumers. If you do this, make sure to remember not to mount the storage up to the primary again when it comes back online or you will end up with duplicate messages. Alternately, you can just wait until the downed DC comes back online. As long as there is no storage loss, all of your messages are safe and will be processed. So it really comes down to the SLA (Service-Level Agreement) that you must support, how likely you think a full DC outage is to occur, and how much manual interaction you’re ok with if it does. Well, what if I have to protect against a real DC outage? This is affectionately known in the defense industry as “the smoking-hole scenario”. That is to say, what if I can’t be guaranteed that I’ll be able to migrate storage during an outage because my DC is not simply down, but rather destroyed? Well first things first… Prepare to make some serious performance tradeoffs. I cannot stress this enough! You will not be processing large sets of data while synchronously replicating across a WAN. One solution that I’ve seen customers use is to replicate the storage using some sort of block-level disk replication software. DRBD works well enough in this situation because it gives you a little bit of flexibility over performance vs absolute reliability (look at modes B or C in their docs (not A)). Basically, every write at the filesystem level is a blocking call. That call usually returns as soon as the data has been physically written to the disk. In the case of a block-level replication solution, that blocking call will not return until the data has been written to both the primary and the backup disks. Because this all occurs at the filesystem level, it is completely out of ActiveMQ’s hands. It just thinks it’s been given really slow storage. One thing to note here is that you would not set up an ActiveMQ Master/Slave pair using this technology because the filesystem-level locks would not replicate. So you would do the manual failover to the warm site as described above. The difference is that you won’t have to migrate the storage as it’s already been replicated safely to the backup DC. The second set of solutions that I’ve seen are a decent tick faster (still not blazing though), but add a bit complication to the clients. They’re both based on some variation of fanout/multicast. The basic gist of it is that the producer clients will send a copy of every message to a broker on both DCs. This isn’t that bad for the producers since the Fanout transport handles all of the work for them. In fact, you can even toss in ActiveMQ’s Proxy Connector and the producer clients won’t have to change a bit. I know what you’re thinking… That doesn’t sound too bad. Where’s all this “complication” you were rambling on about? Don’t get too excited. We just haven’t gotten there yet. The added complication comes on the consumer/processing side. Basically, I now have duplicate messages that I’m processing. And I either have to prevent them using something like Idempotent Consumers or embrace them by adding complication to my code. The idempotent consumer strategy looks attractive because Camel makes it extremely easy. I just have to set up some sort of shared org.apache.camel.spi.IdempotentRepository and add a few lines of Camel routing to my JMS consumers. Take a look at the https://github.com/joshdreagan/clustered-amq-examples example (specifically the “clustered-amq-examples-idempotent” module) for more details. The problem is that I have to set up a repository that synchronously replicates its data across a WAN. So aren’t I back to the same exact problem I had before? Well… not quite. Definitely similar though. I’ll get a small performance boost since the producers are multicasting the message to both brokers in parallel (but blocking until they get back the configured minimum number of “acks”). And the synchronous replication that I mentioned is only an idempotent key (ie, the JMSMessageID) and not the entire message itself. Also, all instances are active. So I’m getting a bit more loadbalancing for my consumer clients. And they can failover faster than if I had a warm site setup (since I don’t have to perform a manual failover). That being said, don’t expect the performance to be stellar. But what if I want my performance to be stellar? I told you you can’t have it! But we can possibly get a tiny bit better. The second option that I mentioned is to embrace the duplicates. Let’s expand on that. If I multicast/fanout a copy of every message to both brokers, I’m going to take that hit. If I want a fully synchronous, total data backup, there’s just no getting around it. But I don’t necessarily have to have my consumers coordinate. What if I just went ahead and processed the duplicates? Would the world end? Let’s use a concrete example. Let’s pretend that my consumers were accepting messages and placing each one into a relational DB. And let’s say that I might have some REST service that, when invoked with some ID, returns a record for that ID from that relational DB. If I inserted both copies of the message, my REST service would return two records. That’s not ideal! But what if it didn’t? What if I just had my REST service run a ever-so-slightly more complicated query that just returned the first record it found? I don’t care which one. They’re both the same. Now I’m back to the result that I wanted. And I have a pretty robust system that can handle duplicate records (both intentional and unintentional). Not too shabby! Now since you’re never satisfied, you might be asking about all of that duplicated data taking up extra space. Well that’s easy enough to fix… Simply have a “cleanup” job that runs at some predifined interval and removes any duplicates that it finds. I don’t care when it runs, or which record it ends up removing. My application will always return the correct result. Take a look at the https://github.com/joshdreagan/clustered-amq-examples example (specifically the “clustered-amq-examples-dupsok” module) for more details. So now you see why I saved JMS (ActiveMQ) HA to the very end. It’s a complicated subject with lots of possible solutions. Hopefully, one of them meets your needs. If not, there is just no pleasing you… Either way, this blog post has become way too wordy. So I’m calling it a day. :)","link":"/2016/07/28/ha_deployments_with_fuse/"},{"title":"Ordered Messaging With ActiveMQ &amp; Camel","text":"I’ve been to several customers over the years who have a requirement to consume messages from a JMS queue in an ordered fashion. The discussions always go the same way… It starts out as a simple design, but becomes really problematic when they get to the implementation phase. Turns out, it’s really not all that simple once you try to scale. In this blog post, we’ll explore in a bit more detail and give some possible solutions. The sample code for this blog can be found at https://github.com/joshdreagan/ordered-activemq-consumer. Typical ArchitectureSo the first thing that people do is to create some test code. They know that JMS queues preserve order. So the logic goes that, if I put messages on the queue in order, I should be able to pull them off in the same order. They end up with something that looks similar to the picture below. Run a test and you’ll see that it does indeed work. Ship it! Well… maybe not just yet… ProblemsThis architecture may work in a very simple use case, but it has some serious limitations. The first problem that you’ll encounter is that this setup is really slow. It’s a given that, in order to process messages in order, the processing must occur in a single thread. However, I will likely have more than one group that could be processed independently. So in theory, I can have multiple producers sending sequences of messages. Take a look at the illustration below. Technically, everything still works. The single consumer will receive both sequences of messages and will process them in the order he receives them. So in the end, both groups will have their messages processed in order. But hopefully the flaw in this setup is obvious. As we scale up and add more and more producer groups, we are bottlenecked in performance by the single consumer. What happens when we try to scale the consumers? Things no longer work. Because the broker is going to loadbalance messages to the available consumers, we have no guarantee that the sequences of messages will be processed on the same thread. Which means we can’t guarantee order. And since we can’t add more consumers, there’s no sense in scaling out and adding more brokers either. Bummer… :( Possible SolutionsSo the question is… How do we scale things out while maintaining our ability to process groups of messages in order? Well, one solution you might consider is ActiveMQ’s Message Groups. Basically, it’s a really neat feature that allows you to do “sticky” loadbalancing of messages to the available consumers. If the producers include a JMSXGroupID header on the JMS message, the broker will check to see if there is a consumer available that has already received messages with that same JMSXGroupID. If one is available, it will deliver the message to it (and all subsequent messages as well as long as it remains available). If not (either because this is the first it has seen that JMSXGroupID, or because the previous consumer has gone away), it will pick a new one. Here’s how that might look: Problem solved right? Depends on if you only need to use a single broker. And if you want to use more than one, it depends on how strict you are about the message ordering. Most of the time, things works fine. And if “most of the time” is good enough for your requirements, then this is definitely the simplest solution. So go with it. But let’s go ahead and discuss the corner case where it doesn’t work out so well. There are 2 things to be aware of: First, when a message is received by a broker, it is persisted in that broker’s store and exists only on that broker. Second, if a consumer goes away for any reason (ie, network blip, restart, …) the broker will pick a new recipient and start sending the messages to it instead. So let’s assume that I have a network of brokers setup (because I like to scale). And if I have a network of brokers setup, I’ll probably use some form of failover (because I like to be HA for my clients). So in this setup, what happens if we send a message to a broker, but the broker is taken down before it can deliver it. The producers would failover and keep sending messages to the next available broker. That broker would pick a consumer and continue delivering messages to it with no knowledge that the failed broker was still holding on to some messages. Now I’m back to getting my messages out of order. Here’s what that might look like: Furthermore, what if I don’t have control over the producers? If I don’t control them, I might not be able to enforce that they set a JMSXGroupID header. So how might I go about solving this conundrum? I’m glad you asked. :) One solution would be to use some Camel magic. Because Camel is awesome! If I use Camel as a consumer, I can have it pipe the messages into an aggregator that can just store them up in a list. Once I’ve received all of them, I can then send them to a splitter to get them back to individual messages, and then send those individual messages through a resequencer (and finally to my desired destination). My messages for each group/sequence will be processed in a single thread (assuming I don’t enable any parallel processing) and will be emitted in order. EIPs for the win! The nice thing about this solution is that I don’t have to worry about those stuck messages. As soon as the failed broker comes back online (or its slave takes over), I will receive that stuck message. And until it does, all of my other messages for that group/sequence will sit patiently and wait inside my aggregation repository. Another thing to note is that I don’t have to complete my aggregation based on some fixed number. I have all kinds of flexibility for my criteria. Take a look at the Camel docs for more info. Here’s a nice picture: So what’s the catch? Well… technically all of the aggregation repository implementations that exist so far can’t work in a cluster or even with multiple threads. There has been some work to handle optimistic locking, but if you give it a try (or dig through the code if you don’t believe me) you’ll find that they only handle the case of 2 threads trying to do the initial insert at the same time. They still have an issue where 2 threads might be attempting to update the repository at the same time and could squash eachother’s changes. Luckily, Camel lets us write our own org.apache.camel.spi.AggregationRepository implementations. Did I mention that Camel rocks!? Take a look at the example code found here: https://github.com/joshdreagan/ordered-activemq-consumer. Basically, I just copied most of the code from the existing org.apache.camel.processor.aggregate.jdbc.JdbcAggregationRepository implementation. But I added a version column to the underlying database tables and some logic to check and increment it (or throw an exception if it doesn’t match) on insert/update. I’m sure there’s a lot more that could be done to make it more robust (like have it implement org.apache.camel.spi.RecoverableAggregationRepository as well). But hey… this is just an example. Do your own coding damnit! I’m sure this isn’t an exhaustive list of every possible way to solve this problem. But it’s a least a couple… And that should definitely be worth a glass of good Scotch. So if you see me at Red Hat Summit… :)","link":"/2016/05/27/ordered_messaging_with_activemq_and_camel/"},{"title":"Scaling JBoss A-MQ on OpenShift","text":"I frequently get asked by customers if it’s possible to run Red Hat JBoss A-MQ on Red Hat OpenShift, and while the answer has been “yes” for quite a while now, it has always been followed by a few caveats. In particular, the issue of scaling… But before we get into the issue of scaling, let’s talk a little about how the official image template works. Basically, it can operate in three different modes (as of this blog date). The first, is persistent with no scaling. This is the equivalent of a single master/slave setup. Only, there is no need for an actual “slave” instance. If the master goes down, OpenShift will detect it and will spin up a new instance on the same or another node. And since the new instance/pod will have the same PersistentVolumeClaim, the broker will come online and see all of its in-flight messages exactly as they were. If I were to attempt to scale-up the instance in this mode, I would basically just spin up a bunch of passive “slaves” since they’ll all try to mount the same PersistentVolumeClaim/KahaDB, and will be unable to get the file-lock (and will resort to polling it until they can). And as described above, the slaves serve no real purpose because OpenShift is already monitoring and will bring up a new instance if needed. But what if I want to scale? What if I want to have many active instances sharing the load? That brings us to the next mode. Which is non-persistent with scaling. In this mode, all of the instances share the same Service, but have no PersistentVolume attached. That means that clients (both producers and consumers) can be distributed across all of the instances. And, since each of the instances is networked together, the messages will find their way to a valid consumer. The instances will be automatically network together in a mesh configuration, and discover eachother using the same Kubernetes Service abstraction that the clients can use. So theoretically, I can scale this up as large as I need to handle my client load. But as I stated above, there is no PersistentVolume attached. Which means that my in-flight messages could potentially be lost if the owning broker goes down. So what if I want it all? What if I need the ability to scale-up, but also need persistence? In that case, we would use the third mode. Which is persistent with scaling. In this mode, all of the brokers are networked together (using the same Kubernetes Service discovery mechanism as above), but they all also mount the same PersistentVolumeClaim. So how do they have separate KahaDB’s (and prevent all trying to lock the same one)? It’s actually quite simple… In this mode, they will all mount the same volume, but will use subdirectories inside that mount. So within the mount, you will get a bunch of directories called “split-1”, “split-2”, … and so on. If you want to see exactly how this works, you can open up a remote shell to one of the pods (ie, oc rsh &lt;POD_NAME&gt;) and take a look at the A-MQ start script. It just loops through (starting at 1) each of the directories until it finds one that it can get a file-lock on. Once it does, it starts up a broker instance and uses that sub-directory to store its KahaDB. It’s worth noting here that, since all of the instances will share the same actual PersistentVolume (and it’s underlying filesystem), you will need to use a distributed filesystem (ie, GlusterFS) with ReadWriteMany access so you don’t hit a storage performance bottleneck. Now I can run A-MQ on OpenShift and scale-up as much as I want (or as much as I have resources for anyway). So what’s the “scaling problem” I mentioned earlier? Well… if I want to scale-up, I’ll probably also want to scale-down at some point. And if I scale down, I now have KahaDB’s sitting in “split-x” folders that could potentially have in-flight messages. And those messages likely can’t wait until I scale back up and happen to get an instance that mounts that particular “split” directory. So really, I need to drain those messages out of that stale KahaDB and push them to one of my remaining active brokers. So how might I go about that? One solution might be to use a broker plugin that (on shutdown) will automatically drain messages off to other brokers. This could work, but would probably be problematic. First, you would have to make sure that all of your ActiveMQ TransportConnectors are shutdown before you start draining the messages. If you fail to do this, you could potentially be accepting new messages from clients and might never finish actually draining. The second (and probably more important problem) is that you don’t have an infinite amount of time to finish your work. When Kubernetes schedules a pod for shutdown, it does give a max time for it to complete its graceful shutdown. But if you exceed this timeout, it will forcefully shut down. So exactly how much time do you need to drain off all of your messages? It depends… It depends on how many in-flight messages you have stored in that KahaDB. It depends on how fast you can send those messages (maybe they’re large messages). It depends on how much space you have available on the other brokers (because of Producer Flow Control). The point is, there is no valid number for “shutdown timeout”. You need as much time as it takes. So what do we do? Luckily for you, I’m sure you’ve already read my previous blog on Decommissioning JBoss A-MQ brokers. And in there, you’ve already seen my final proposed solution (and code example) for draining those messages. So really, all we have to do is make that code work on OpenShift. Here’s a first cut at it [https://github.com/joshdreagan/activemq-pv-monitor]. In this example, I use the FIS 2.0 tools to create a simple Red Hat JBoss Fuse app that will monitor the A-MQ “split-x” directories. If it finds a KahaDB that it’s able to get a file-lock on, it will drain its messages to another available broker (which it discovers using the same Kubernetes Service discovery mechanism described above). And since it’s a separate Pod, it can run for as long as it needs to. So no need to worry about pesky timeouts. The example could use some more error handling and various other QA, but it should be a good starting point. So now we can scale-up, we can scale-down, and if we’re feeling lazy, we can even auto-scale. Cool beans! As always, hopefully you find this useful. And if so, buy me a beer this year at Red Hat Summit. :)","link":"/2017/03/25/scaling_jboss_a-mq_on_openshift/"},{"title":"Smart LoadBalancing With Camel","text":"LoadBalancing is a fairly well-known concept these days. There are a ton of existing strategies out there (ie, RoundRobin, Random, Sticky, Weighted, …), and there a ton of existing implementations that have been built using both hardware and software (ie, Apache HTTPD, HAProxy, f5, Layer7, …). So why create another one? Well… although it’s not likely to be very useful, I thought it might be neat to see if I could make one that utilized CPU load (or any metric) to do more intelligent routing. Luckily, like many things in Camel, this is a fairly simple task. I just have to create my own org.apache.camel.processor.loadbalancer.LoadBalancer implementation, and I can make it do pretty much anything I want. For instance, I might implement one to do dynamic discovery using Infinispan (shameless self-promotion :)). But I digress… So let’s break down the wish list: I want to be able to use the strategy for more than just HTTP. I’d like to be able to use any available metric. And I need the collection of said metric to occur asynchronously in the background (so I don’t slow down my routing). Take a look at the source code https://github.com/joshdreagan/camel-smart-loadbalancer to see how I did it. In my example, I load balanced HTTP calls and used JMX to collect CPU utilization. But you could just as easily use the exact same implementation to monitor ActiveMQ queue depth (or queue % full) and load balance between brokers. Or maybe monitor filesystem space and load balance FTP endpoints. Like I said in the intro, this is probably not terribly useful in a real-world environment since the metrics will likely change at a faster rate than you would reasonably poll. But at the very least, hopefully someone will find it interesting. And maybe… just maybe… it will get Christian Posta to read my blog. :)","link":"/2016/10/10/smart_loadbalancing_with_camel/"},{"title":"Transactions and Alternatives With Camel","text":"There are loads of use cases which require “all or nothing” processing. And there are a bunch of different strategies for accomplishing said result. Luckily for me, they’ve already been covered many times in tons of different blogs/books/articles. So for this post I’m just going to concentrate on a few of the strategies, and more specifically, how to do them with Apache Camel. TransactionsThe first (and most obvious) solution that I’d like to cover, is transactions. Usually, when people need to do a bunch of tasks in an atomic fashion, they simply use a transaction. This can be either a local transaction, or an XA one. Basically, you get to pawn off all of the complication onto a transaction manager and keep your application code clean. So it’s a great option if you’re using resources that can be transacted (ie, a relational database, or a JMS broker). If you’re only using a single resource, you can do a local transaction. Which is a nice balance of simplicity and speed. For instance, consuming from a queue, enriching with some extra data, and then producing to another queue on the same broker. The only thing you have to be cautious of is that you maintain a single thread throughout your processing. This really only gets tricky if you do something like a Splitter EIP and turn on the parallelProcessing option. If you need an example of configuring a local transaction, you can just take a look at the docs [https://camel.apache.org/components/latest/eips/transactional-client.html]. However, if you are using multiple resources, you will need to use an XA transaction. For instance, consuming from a queue, and inserting into a database. XA transaction managers are usually provided and configured by your container. If you’re using Red Hat JBoss Fuse on Karaf, you’ll likely use Aries. If you’re on Red Hat JBoss EAP, you’ll use Narayana (formerly JBoss TM). Spring Boot provides no transaction manager out-of-the-box. Instead, it has hooks to auto-configure various TM’s based on which one you’ve added as a dependency. In case you’re curious and would like an example of Camel + XA + Spring Boot, take a look here [https://github.com/joshdreagan/camel-spring-boot-xa]. Using an XA transaction manager does increase complexity a bit (at least from a configuration perspective), and comes with a handful of requirements and caveats: The first requirement, is that you will (obviously) need to run and configure some sort of XA capable transaction manager. There are several options available (as outlined above). And because they all implement JTA, you can swap them out with no changes to your code. So you can shop around and find the one that works best. Second, due to the requirement of a 2-phase commit, it will be significantly slower. This is usually a huge sticking point for a lot of people as they don’t want to (or can’t afford to) pay that performance penalty. Unfortunately, there is little that can be done about it. Or more accurately, I have never seen an XA transaction manager implementation that maintains the speed and simplicity of a non-XA one. The third, and often overlooked, requirement is that you will need some sort of persistence. This is because, in the case of a crash, the recovery manager will attempt to pick up where things left off. And in order to survive a crash, we need persistence… It’s worth noting that this third requirement (persistence) makes HA a bit of a pain. As mentioned above, transaction managers will run some sort of recovery thread in the background so that they can (as the name would suggest) recover transactions that were not yet complete at the time of a crash. But they all (or at least all of the implementations I know of) can only have a single instance of the tx and recovery manager per object store (or more specifically, per tx manager id). So that means that, if I wanted to scale out my application (to make up for the added slowness of XA), each server would likely have its own persistent store. Most people don’t even notice when they’re using a server like JBoss WildFly because each instance will (by default) write its logs to a subfolder of its installation. This can be (in my opinion) very dangerous because most people are unaware that that directory should be sitting on some sort of resilient storage. If, however, you’re running on a platform like OpenShift, you will be immediately aware because all instances will share the same storage mount and configuration, and will simply fail to work properly. You could use subfolders for each pod instance, and then create a separate recovery pod that would run independent of your application and would spin up recovery managers for each downed instance. In fact, I actually had an implementation working at one point. But it was quite clunky, and after a quick conversation with Hiram Chirino at one of our meetups, I concluded that he was working on a way more elegant solution using Stateful Sets. So for now, if you want to run XA transactions on OpenShift, I would either limit my app to a single instance (ie, no scaling), or wait a bit for Hiram’s version. Idempotent ConsumerSo what if I can’t (or don’t want to) use XA transactions? Perhaps I value speed over application simplicity… Perhaps I’m not dealing with “transactable” resources… Perhaps I’m running on OpenShift and can’t wait on Hiram… ;) No matter what the reason, it’d be nice to have some other options. Luckily, as mentioned at the beginning of the post, there are oodles of options. So lets talk about one of the more popular ones… idempotent consumer. With the Idempotent Consumer EIP, you give up on trying to do things atomically, and instead favor eventual consistency. In other words, I write my application in such a way that retries will not hurt anything. So if I have a failure, I can just keep retrying until I eventually succeed all the way through. To give a concrete example… Let’s say that I wanted to read a file from an input directory, unmarshal/validate the contents, insert it as a record into a DB, and also write it out to a file in an output directory (perhaps in parallel). In this example, only the DB write can participate in a transaction. Both the consumption of and the production of files cannot. So local (or even XA) transactions are not an option. But if I put a simple check before writing the DB and also before writing the file, I can retry as many time as I’d like with no negative side-effects. To be very specific, if I was successful in writing to the DB, but failed on writing the file (maybe because the filesystem was temporarily full), I can just re-ingest the same file. The DB check will ensure that I don’t try that step again. So I’ll basically skip it and then try the file write again. Hopefully this time it succeeds… There are a couple of ways that I can go about implementing this with Camel. One way would be to guard each step using the Idempotent Consumer EIP. With this EIP, I select a unique id (or rather, an expression to retrieve a unique id) for each message. When that message is received, its unique id will checked against an org.apache.camel.spi.IdempotentRepository (of which there are many implementations to choose from). If it already exists, it will simply be skipped. If not, it will be added and then passed on to the processor. Now, I can ingest/retry my data as many times as I want and be relatively certain that each step will only be performed once. If you want an example of this pattern, take a look here: [https://github.com/joshdreagan/camel-idempotent-consumer]. This pattern works great for most cases. It’s easy to implement, and maintains pretty good performance. But sometimes it just isn’t flexible or robust enough. More specifically, what do I do if I don’t have a unique message id to key off of? Also, what happens if I have a system failure after adding something to the idempotent repository, but before performing my actual processing? Or what if I have an error during my processing, but suffer a system failure before I can remove the message id from my repository? Basically, I have situations where my idempotent repository could be out-of-sync with my actual processing. If I’m using the JDBC based implementation to guard a DB insert, I could use a local transaction to make sure both of those steps occur atomically. But that doesn’t really help with my file writing use case. The margin for error is pretty small, and might be acceptable. But what if it’s not? Luckily for us, Camel usually has more than one way to solve a problem… The Idempotent Consumer EIP is really just a specialized version of the Message Filter EIP. The biggest difference, is that its expression will return a boolean dictating whether or not it skips or processes the message. So instead of just matching an id, I can perform any steps I’d like to determine if a message has already been processed before. Which solves my first issue… Using my example above, I would guard my DB insert with one filter check, and then my file write with another filter check. The DB filter check could query the DB to see if my message had already been inserted, and then skip processing if it had. My file writer check could similarly check to see if the destination file had already been written, and then skip processing if it had. It’s a tiny bit more complicated to use, but since I’m not maintaining a separate idempotent repository, I don’t have any chance of getting out-of-sync. So that solves my system failure issues… If you want an example of what this might look like, take a look at this example: [https://github.com/joshdreagan/camel-filter-consumer]. As I said at the beginning of this post, there are several ways to tackle this issue. Of which, I’ve only covered a few. And in doing so, I used some fairly contrived use cases. But hopefully it’s still worthwhile, and at a bare minimum maybe someone will find the code examples useful…","link":"/2017/08/14/transactions_and_alternatives_with_camel/"},{"title":"Upgrading AMQ 6 to AMQ 7","text":"So Red Hat AMQ 7 has been out for a while now, and there have already been a lot of customers who are understandably eager to upgrade to the latest and greatest version. Many of them have been reaching out and asking for help/instructions on how to migrate. So far I’ve been replying that I already blogged about that here: Decommissioning JBoss A-MQ brokers. But that reply has been met with some confusion. So I figured I’d write up a more concrete set of instructions. Let’s start by stating the problem: “I have a current, production system that is utilizing AMQ 6 and I’d like to upgrade it to AMQ 7 with as little downtime as possible”. Well… such things do require a bit of planning, but it’s not as difficult as it might seem. The easiest way to upgrade any app is usually to do a rolling upgrade. I emphasize “usually“ because every customer has a slightly different case, architecture, requirements, or other wrenches that can make things more difficult. But for the purpose of this blog, let’s focus on the most typical case. For the first step, let’s walk through upgrading the brokers. First, we’ll need to install AMQ 7 and create a broker instance. No need to enumerate the steps here since that’s already been covered in the docs: [https://access.redhat.com/documentation/en-us/red_hat_jboss_amq/7.0/html/using_amq_broker/]. For the purpose of this post, we’ll assume that you plan to match your existing setup (ie, if you currently have a master/slave pair, you will install a new AMQ 7 master/slave pair). If you’re installing to new hardware, you can go ahead and start the newly created broker instance(s). Otherwise, we’ll have to shut down the existing AMQ 6 broker before starting the AMQ 7 broker. Yes… you could modify the ports and bring it up alongside your AMQ 6 broker, but you risk causing a contention for resources if you do so. So best not to tempt fate… Next, let’s talk about the clients. One of the awesome features in AMQ 7 is that is supports all of the same protocols that AMQ 6 did (in addition to a couple more). This means that your existing clients (with their existing client libraries) can seamlessly connect to the new AMQ 7 brokers. All you’ll need to do is give the clients the new broker URL and they can immediately begin producing/consuming. And you don’t even need to do that if you installed to the same hardware and bound to the same port. In fact, if your clients are using the “failover” protocol (and you didn’t update the host/port), they will automatically switch over as soon as you take down the AMQ 6 broker and bring the AMQ 7 broker online. Neat! It’s worth noting that this will not be the case forever. Eventually, the OpenWire format (and potentially other formats) will be deprecated and removed from support. But that is a long ways away. So you’ll have plenty of time to go back through and update all of your client applications with the newer client libraries as time/budget permits. But what about those in-flight messages? The messages that had been accepted by the old AMQ 6 instance, but had not yet been delivered to a consumer client. Well, that is exactly what I wrote my previous blog post about. Those messages cannot be lost. So we need to “drain” them from the old AMQ 6 KahaDB store and send them to the new AMQ 7 broker. Luckily, due to the fact that the AMQ 7 broker can still speak OpenWire, you can use the exact same drainer code that I provided in that blog: [https://github.com/joshdreagan/activemq-drainer]. Super neat! That’s it! Rinse and repeat for each broker instance/pair. You can do it all at once, or in a “rolling” fashion. Up to you… So to summarize, here are the high-level steps that you will perform: Install the new AMQ 7 broker/instance. Stop the AMQ 6 broker. Start the AMQ 7 broker. Update your clients with the new broker URL (only necessary if you installed to new hardware or otherwise changed the host/port). Drain the messages from the AMQ 6 KahaDB to the new AMQ 7 broker. Eventually plan on upgrading your client libraries. Optionally delete the old AMQ 6 installation once you’re satisfied that the upgrade has completed successfully. Did I cover every possible use case, permutation, and complication? No… But this should be a good starting point. And if you need more guidance, well… that’s what we have Red Hat Consulting for.","link":"/2017/12/01/upgrading_amq_6_to_amq_7/"},{"title":"Streaming in the Cloud With Camel and Strimzi","text":"So I was at one of my favorite customers several months back, and was demo’ing our new AMQ Streams product (aka Strimzi). They asked for some example Apache Camel apps to show them how to securely connect to their newly installed Kafka on OpenShift. Not an unreasonable request… But try as I might, I could not find any existing examples. Sure, I found examples of Camel talking to Kafka, but not doing so securely. I found examples of Camel talking to Kafka with authz, but not running in OpenShift. I even found examples of plain Java clients running in OpenShift and doing authz, but not using Camel. So I offered to create a set of examples for them “as soon as I got some free time”. Six months later… here we go! ;) For the purpose of this blog, let’s go ahead and assume that you’ve already installed OpenShift. Let’s also assume that you’ve installed Strimzi onto your OpenShift environment and have a Kafka cluster running. There’s no need to cover either of those topics since they’re both thoroughly outlined in the existing documentation. Instead, let’s focus a bit on authentication and authorization. And more specifically, how those work in Strimzi. At present, there are two mechanisms for authentication (TLS or SCRAM-SHA-512), and one for authorization (simple). So we’ll need to both enable “simple” authorization on the broker, and choose/enable one of the authorization mechanisms on the listener. It’s actually really easy! You can just follow the docs here. So now my brokers expect me to authenticate? Great… How do I do that? Well, you’ll need to create and apply a “KafkaUser” definition. In said “KafkaUser” definition, you’ll specify both the authentication type (which should match the authentication type that you’ve chosen for your listener), and the authorization roles. The authorization roles are what gives permissions to a specific resource. So, for instance, whether or not a user has access to read or write to a topic. They are detailed in the docs here. Once you’ve applied your “KafkaUser” definition to OpenShift, the “User Operator“ will detect it and generate some resources for you automagically. Neat! But what does it generate? Well that depends on the authentication type you’ve chosen. Let’s start with SCRAM since it’s the easiest. SCRAM-SHA-512 When you create a “KafkaUser” with a “scram-sha-512” authentication type, the “User Operator” will generate an OpenShift “Secret” with the same name. So, if I’ve defined a user named “bob”, I will see a secret named “bob” that has a key called “password”. Those are the credentials that I’ll use to connect. Seems straighforward… But how do I get that into my app? The easiest way is to inject the password into an environment variable in your container. You can do this in your deployment.yml file for your app. Here’s an example: deployment.yml1234567#...- name: KAFKA_USER_PASSWORD valueFrom: secretKeyRef: name: bob key: password#... If I include the above snippet into my deployment.yml, I will have a KAFKA_USER_PASSWORD environment variable (containing my generated password) available to me inside of my container. So I can just reference it in my application.yml file like below. application.yml12345678#...camel: component: kafka.configuration: security-protocol: SASL_PLAINTEXT sasl-mechanism: SCRAM-SHA-512 sasl-jaas-config: org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;bob&quot; password=&quot;${KAFKA_USER_PASSWORD}&quot;;#... Now, when my Camel app connects to Kafka, it will connect as my specified user, using the auto-generated password. And, if desired, I can have OpenShift trigger a reload of my app if the password is updated/regenerated. Cool beans! On to a slightly more complex case… TLS When using “tls” as my authentication type, I (as a client) will need both the broker’s public key, as well as my user’s public &amp; private keys. Such is the nature of mutual auth… Similarly though, those will be generated for me by the various operators. But it’s a little more complicated than the SCRAM case. How so? Well, just like before, I can inject the secret values into environment variables as shown below. So no issues yet. deployment.yml1234567891011121314151617#...- name: KAFKA_CLUSTER_CRT valueFrom: secretKeyRef: name: my-cluster-cluster-ca-cert key: ca.crt- name: KAFKA_USER_CRT valueFrom: secretKeyRef: name: alice key: user.crt- name: KAFKA_USER_KEY valueFrom: secretKeyRef: name: alice key: user.key#... But I can’t just use those key/cert values directly. I actually need to create a keystore &amp; truststore, and then import those keys into their appropriate stores. Well, how do I go about that? One solution would be to use a custom container image and start script as per Jakub’s example. While this solution is very clever (as is Jakub :)), I wondered if there was a way to do it all within my app code. A quick Google search and, lo and behold, I find env-keystore! This handy little library will let me easily create Java keystores using arbitrary keys/certs from string values. So now I can actually use those injected environment variable values. What’s more, it can write the stores out to a file once I’ve created them. And since it uses Bouncy Castle, it can handle both Java formatted keys/certs as well as OpenSSL formatted ones (which is what Strimzi will generate for you). Good stuff! So if I include the below dependency… pom.xml12345&lt;dependency&gt; &lt;groupId&gt;com.heroku.sdk&lt;/groupId&gt; &lt;artifactId&gt;env-keystore&lt;/artifactId&gt; &lt;version&gt;x.x.x&lt;/version&gt;&lt;/dependency&gt; And add a little bit of initialization code… KafkaComponentCustomizer.java12345678910111213141516171819202122232425package org.apache.camel.examples;//import ...@Component public class KafkaComponentCustomizer implements ComponentCustomizer&lt;KafkaComponent&gt; { @Autowired private KafkaComponentConfiguration kafkaConfiguration; @Value(&quot;#{systemEnvironment['KAFKA_CLUSTER_CRT']}&quot;) private String kafkaClusterCrt; @Value(&quot;#{systemEnvironment['KAFKA_USER_KEY']}&quot;) private String kafkaUserKey; @Value(&quot;#{systemEnvironment['KAFKA_USER_CRT']}&quot;) private String kafkaUserCrt; @Override public void customize(KafkaComponent component) { try { BasicKeyStore truststore = new BasicKeyStore(kafkaClusterCrt, kafkaConfiguration.getConfiguration().getSslTruststorePassword()); truststore.store(Paths.get(kafkaConfiguration.getConfiguration().getSslTruststoreLocation())); BasicKeyStore keystore = new BasicKeyStore(kafkaUserKey, kafkaUserCrt, kafkaConfiguration.getConfiguration().getSslKeystorePassword()); keystore.store(Paths.get(kafkaConfiguration.getConfiguration().getSslKeystoreLocation())); } catch (IOException | GeneralSecurityException e) { throw new RuntimeException(e); } }} My app can now grab those generated keys/certs from the OpenShift secrets, inject them into env variables, use those values to generate the client keystore/trustore, then reference those stores when it makes its connection to the brokers. More difficult than SCRAM, but still not too bad. Although… Syncing the secrets When the various operators generate secrets for you, they will do so in the namespace where the resources live. That’s all fine and good if my apps are colocated alongside my Kafka cluster. But what if I want to put my cluster in one namespace (let’s say one called “strimzi”), and my client apps in another (let’s say it’s called “fuse”)? Unfortunately, OpenShift currently won’t allow me to access secrets between namespaces. So I’d have to copy the values from the generated secrets in my “strimzi” namespace into manually created secrets in my “fuse” namespace where my client apps live. That sounds super error prone. What do I do if those secrets update/regenerate? Certificates do expire right? I’ll have to make sure that when they update, I’ll go update all the copies. That’s a recipe for disaster! If only there was a way to syncronize those secrets between namespaces automatically. A little bit of searching and you’ll likely stumble across AppsCode Kubed. Kubed is an operator that, once installed, will monitor and sync any secrets you specify (among other things). It will keep them updated if the source secret changes, and even remove the copies if the source secret is deleted. And it can do this across as many namespaces as you need. So no need to worry if you have multiple apps in multiple namespaces. Perfect right!? Well, one minor hitch… The way Kubed works, you have to apply an OpenShift annotation to the secret that you want sync’d. It will then find and monitor any secrets with said annotation, and then sync them to namespaces that have the appropriate label. Unfortunately, if the Strimzi operators see that the secret has been changed in any way (like say, adding an annotation), they will squash those changes and get things back in sync with their configs. Which really is what they should do in that case… There’s currently a JIRA issue open for adding the ability to specify annotations on the generated secrets via the Strimzi configs. And once that enhancement is made, Kubed will definitely be the way to go. But what can we do in the meantime? It’s not great, but one option is to use a “CronJob” that will simply execute some bash commands to sync the secrets. Clunky? Yes. Ideal? No. Works? Yes. Here’s an example one that I wrote to sync the broker’s public key. I installed it into the “fuse” namespace, and it sync’d my secret from the “strimzi” namespace, as needed, every second. secret-sync-cronjob.yml1234567891011121314151617apiVersion: batch/v1beta1kind: CronJobmetadata: name: camel-kafka-authz-secret-syncspec: schedule: &quot;* * * * *&quot; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: serviceAccountName: camel-kafka-authz-sa restartPolicy: Never containers: - name: cluster-ca-cert-sync image: openshift/origin-cli command: [&quot;bash&quot;, &quot;-c&quot;, &quot;export SRC=\\&quot;$(oc extract -n strimzi secrets/my-cluster-cluster-ca-cert --keys=ca.crt --to=-)\\&quot;; export DST=\\&quot;$(oc extract -n fuse secrets/my-cluster-cluster-ca-cert --keys=ca.crt --to=-)\\&quot;; if [ -n \\&quot;$SRC\\&quot; ] &amp;&amp; [ \\&quot;$DST\\&quot; != \\&quot;$SRC\\&quot; ]; then echo 'Values differ. Syncing...'; oc create secret generic -n fuse --dry-run -o yaml my-cluster-cluster-ca-cert --from-literal=\\&quot;ca.crt=$SRC\\&quot; | oc apply -f -; fi;&quot;] So once again, we’ve managed to solve all the worlds problems. Or maybe none of them… :) Either way, if you’re looking for more than just snippets, take a look at the full source code for this example: https://github.com/joshdreagan/camel-kafka-authz.","link":"/2019/05/30/streaming_in_the_cloud_with_camel_and_strimzi/"},{"title":"Getting Started - Camel on EAP","text":"This guide demonstrates step-by-step how to create a sample Camel project in JBoss Developer Studio (JBDS) using the Maven archetypes that come out of the box. Specifically, we’ll be creating a contract-first based web services java project. We’ll then convert that project to a WAR file so that it can be deployed to JBoss EAP. Finally, we’ll deploy the project and verify that it is running. Here are a few assumptions before you begin: You already have a supported JDK installed (ie, Oracle’s JDK version 6 or 7) You already have Maven version 3.x installed You already have JDBS installed (tested with version 7.1.1, but should work with newer) You already have the “JBoss Integration and SOA Development” tools for JBDS installed You already have JBoss EAP installed (tested with version 6.3.0, but should work with others) After you’ve met all the prerequisites, you can proceed with the guide. Step 1: Create A New Fuse ProjectStart off by creating a new project. Click File-&gt;New-&gt;Project… to start the “New Project” wizard. Select “Fuse Project“ as the project type and click “Next &gt;“. Specify your preferred project location (or let it use the default) and click “Next &gt;“. Select the appropriate Maven archetype from the list. The one we use for this example is “io.fabric8:camel-cxf-contract-first-arch:1.0.0.redhat-379“. Enter your preferred Group Id, Artifact Id, Version, &amp; Package. When done, click “Finish“. Your new project should be created. If this is the first time, it can take a few minutes as JBDS downloads all the required Maven dependencies. Step 2: Convert Project To WAR FormatFirst we need to change the packaging type in the Maven POM so that we will build a WAR file instead of the default JAR file. To do so, open up the project’s pom.xml file, click on the drop-down list next to “Packaging:“, and select “war“ as the type. Save your changes when done. Next, we need to tell the Eclipse Maven Plugin to update the project configuration so that JBDS will add the appropriate facets and display the project structure correctly. So first, right-click on the project and select “Maven-&gt;Update Project…“ to start the “Update Maven Project” wizard. Make sure the project is selected and click “OK“. You should notice that the project structure changes a bit. Now we will need to add some Maven dependencies so that we can bootstrap our Camel Context in a Servlet container. Open up the project’s pom.xml file, and select the “Dependencies“ tab. Click on the “Add…“ button to start the “Select Dependency” wizard. Fill out the input boxes as shown below and click “OK“ when done. Don’t forget to save your changes. Once we have all of our dependencies in place, we can create our deployment descriptor (ie, the web.xml file). So right-click on the “Deployment Descriptor:“ section in the project view and select “Generate Deployment Descriptor Stub“. This will create a skeleton web.xml file that you can use as a starting point. Now double-click on the “Deployment Descriptor:“ section in the project view to edit the web.xml file. We will need to add several elements. A context-param element to tell Spring where the XML files are located. A listener element to load Spring’s ServletContextListener which will bind the Spring Context to the Servlet’s lifecycle. A servlet element to name the CXF Servlet. A servlet-mapping element to map the CXF Servlet to a URL pattern. If you’d like, you can just copy/paste the following XML: 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; version=&quot;3.0&quot;&gt; &lt;display-name&gt;camel-cxf-contract-first&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:META-INF/spring/*.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;CXFServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.cxf.transport.servlet.CXFServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;CXFServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/soap/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; Save your changes and close the web.xml file. By default, the generated CXF configuration will boot up an embedded Jetty server and bind to “http://localhost:9000/order/“. Since we will be deploying to a server that already has a Servlet container, we can let CXF use the provided one instead. So first, find the camel-cxf.xml file in the “Project Explorer“ tab and open it up. We simply need to change the address attribute of the cxf:cxfEndpoint element from “http://localhost:9000/order/“ to “/order/“. The result should look like the image below. Save your changes and close the file. Finally, you may have noticed that the “Problems“ tab is showing some errors. This is because the Maven archetype that we used has a WSDL that contains 2 minor issues. To fix the issues, we’ll need to edit the WSDL file. To do so, either double-click on the error in the “Problems“ tab, or on the WSDL in the “Project Explorer“ tab to open the order.wsdl file. Now locate the wsdl:binding element. It should be toward the bottom of the file. It should contain wsdl:input and wsdl:output elements. Both of which have a soap:body element with a parts attribute as shown below. Simply remove the parts attribute. The resulting file should look like the image below. Save your changes and close the file. That’s it for the changes. The project should now be ready to deploy to JBoss EAP. Step 3: Deploy Project To JBoss EAPIf you don’t have one already, create a new server configuration in JBDS. Click on the “Servers“ tab. If you don’t have any server configurations, you will see a link like the image below. Click on the link to start the “New Server“ wizard. Select “JBoss Enterprise Application Platform 6.1+“ and click “Next &gt;“. Click the “Browse“ button and navigate to the root directory of your JBoss EAP installation. Click “Next &gt;“ when done. Select your desired options, or leave the defaults and click “Next &gt;“. Select the project from the “Available:“ pane and click the “Add“ button. You should see the project move over to the “Configured:“ pane as shown below. Click the “Finish“ button when done. You should now have a new server configuration as shown below. Next, start the server. You can do so by clicking the icon as shown below. If the server starts successfully, you should see no errors in your “Console“ tab. Finally, open up a web browser (or use the one built into JBDS by clicking the icon) and navigate to “http://localhost:8080/camel-cxf-contract-first/soap/order?wsdl“. If the everything was successful, you should see the WSDL contents displayed as shown in the image below. At this point, you have successfully created and deployed a Camel/CXF web service to JBoss EAP. You can do further testing using a tool like SoapUI (or any SOAP service testing tool).","link":"/2015/04/10/getting_started__camel_on_eap/"}],"tags":[{"name":"activemq","slug":"activemq","link":"/tags/activemq/"},{"name":"amq","slug":"amq","link":"/tags/amq/"},{"name":"fuse","slug":"fuse","link":"/tags/fuse/"},{"name":"camel","slug":"camel","link":"/tags/camel/"},{"name":"spring-boot","slug":"spring-boot","link":"/tags/spring-boot/"},{"name":"artemis","slug":"artemis","link":"/tags/artemis/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"cxf","slug":"cxf","link":"/tags/cxf/"},{"name":"feedhenry","slug":"feedhenry","link":"/tags/feedhenry/"},{"name":"karaf","slug":"karaf","link":"/tags/karaf/"},{"name":"fabric8","slug":"fabric8","link":"/tags/fabric8/"},{"name":"openshift","slug":"openshift","link":"/tags/openshift/"},{"name":"jbpm","slug":"jbpm","link":"/tags/jbpm/"},{"name":"bpms","slug":"bpms","link":"/tags/bpms/"},{"name":"jboss","slug":"jboss","link":"/tags/jboss/"},{"name":"wildfly","slug":"wildfly","link":"/tags/wildfly/"},{"name":"infinispan","slug":"infinispan","link":"/tags/infinispan/"},{"name":"datagrid","slug":"datagrid","link":"/tags/datagrid/"},{"name":"narayana","slug":"narayana","link":"/tags/narayana/"},{"name":"strimzi","slug":"strimzi","link":"/tags/strimzi/"}],"categories":[]}